imgs.shape: torch.Size([64, 6, 256, 256])
v.shape: torch.Size([64, 2])
network: Network(
  (conv1): Conv2d(6, 16, kernel_size=(3, 3), stride=(2, 2))
  (conv1_bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2))
  (conv2_bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))
  (conv3_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=61504, out_features=256, bias=True)
  (out): Linear(in_features=256, out_features=2, bias=True)
)
Begin training loop
epoch 1,	batch    10,	training loss: 803.148
epoch 1,	batch    20,	training loss: 408.008
epoch 1,	batch    30,	training loss: 128.169
epoch 1,	batch    40,	training loss: 63.726
epoch 1,	batch    50,	training loss: 57.802
epoch 1,	batch    60,	training loss: 31.834
epoch 1,	batch    70,	training loss: 30.297
epoch 1,	batch    80,	training loss: 35.406
epoch 1,	batch    90,	training loss: 32.385
epoch 1,	batch   100,	training loss: 33.890
epoch 1,	batch   110,	training loss: 23.346
epoch 1,	batch   120,	training loss: 16.700
epoch 1,	batch   130,	training loss: 23.516
epoch 1,	batch   140,	training loss: 24.420
epoch 1,	batch   150,	training loss: 20.645
epoch 1,	batch   160,	training loss: 20.848
epoch 1,	batch   170,	training loss: 20.576
epoch 1,	batch   180,	training loss: 15.640
epoch 1,	batch   190,	training loss: 19.024
epoch 1,	batch   200,	training loss: 14.478
epoch 1,	batch   210,	training loss: 13.922
epoch 1,	batch   220,	training loss: 17.501
epoch 1,	batch   230,	training loss: 12.107
epoch 1,	batch   240,	training loss: 12.340
epoch 1,	batch   250,	training loss: 7.196
epoch 1,	batch   260,	training loss: 8.034
epoch 1,	batch   270,	training loss: 7.980
epoch 1,	batch   280,	training loss: 9.451
epoch 1,	batch   290,	training loss: 16.073
epoch 1,	batch   300,	training loss: 13.705
epoch 1,	batch   310,	training loss: 13.259
end of epoch 1
testing on validation set:
# correct:  6107/20000 = 30.535%
# off by 1: 5061/20000 = 25.305%
epoch 2,	batch    10,	training loss: 13.808
epoch 2,	batch    20,	training loss: 11.973
epoch 2,	batch    30,	training loss: 7.565
epoch 2,	batch    40,	training loss: 8.251
epoch 2,	batch    50,	training loss: 8.509
epoch 2,	batch    60,	training loss: 9.903
epoch 2,	batch    70,	training loss: 8.680
epoch 2,	batch    80,	training loss: 9.598
epoch 2,	batch    90,	training loss: 10.216
epoch 2,	batch   100,	training loss: 6.854
epoch 2,	batch   110,	training loss: 6.304
epoch 2,	batch   120,	training loss: 7.544
epoch 2,	batch   130,	training loss: 7.850
epoch 2,	batch   140,	training loss: 8.646
epoch 2,	batch   150,	training loss: 9.492
epoch 2,	batch   160,	training loss: 10.873
epoch 2,	batch   170,	training loss: 8.947
epoch 2,	batch   180,	training loss: 6.815
epoch 2,	batch   190,	training loss: 7.313
epoch 2,	batch   200,	training loss: 4.984
epoch 2,	batch   210,	training loss: 7.140
epoch 2,	batch   220,	training loss: 12.022
epoch 2,	batch   230,	training loss: 12.036
epoch 2,	batch   240,	training loss: 7.830
epoch 2,	batch   250,	training loss: 7.099
epoch 2,	batch   260,	training loss: 4.863
epoch 2,	batch   270,	training loss: 3.540
epoch 2,	batch   280,	training loss: 4.857
epoch 2,	batch   290,	training loss: 4.817
epoch 2,	batch   300,	training loss: 5.840
epoch 2,	batch   310,	training loss: 6.002
end of epoch 2
testing on validation set:
# correct:  8575/20000 = 42.875%
# off by 1: 5984/20000 = 29.92%
epoch 3,	batch    10,	training loss: 4.493
epoch 3,	batch    20,	training loss: 12.398
epoch 3,	batch    30,	training loss: 10.571
epoch 3,	batch    40,	training loss: 5.882
epoch 3,	batch    50,	training loss: 6.175
epoch 3,	batch    60,	training loss: 6.286
epoch 3,	batch    70,	training loss: 6.824
epoch 3,	batch    80,	training loss: 6.340
epoch 3,	batch    90,	training loss: 6.425
epoch 3,	batch   100,	training loss: 5.018
epoch 3,	batch   110,	training loss: 3.742
epoch 3,	batch   120,	training loss: 5.812
epoch 3,	batch   130,	training loss: 6.796
epoch 3,	batch   140,	training loss: 5.610
epoch 3,	batch   150,	training loss: 6.182
epoch 3,	batch   160,	training loss: 4.426
epoch 3,	batch   170,	training loss: 3.911
epoch 3,	batch   180,	training loss: 6.045
epoch 3,	batch   190,	training loss: 5.375
epoch 3,	batch   200,	training loss: 5.776
epoch 3,	batch   210,	training loss: 4.456
epoch 3,	batch   220,	training loss: 5.912
epoch 3,	batch   230,	training loss: 6.973
epoch 3,	batch   240,	training loss: 5.739
epoch 3,	batch   250,	training loss: 3.179
epoch 3,	batch   260,	training loss: 3.717
epoch 3,	batch   270,	training loss: 3.467
epoch 3,	batch   280,	training loss: 6.651
epoch 3,	batch   290,	training loss: 5.183
epoch 3,	batch   300,	training loss: 4.384
epoch 3,	batch   310,	training loss: 3.242
end of epoch 3
testing on validation set:
# correct:  10477/20000 = 52.385%
# off by 1: 5970/20000 = 29.85%
epoch 4,	batch    10,	training loss: 3.980
epoch 4,	batch    20,	training loss: 3.412
epoch 4,	batch    30,	training loss: 4.031
epoch 4,	batch    40,	training loss: 3.728
epoch 4,	batch    50,	training loss: 4.021
epoch 4,	batch    60,	training loss: 3.887
epoch 4,	batch    70,	training loss: 4.321
epoch 4,	batch    80,	training loss: 4.104
epoch 4,	batch    90,	training loss: 3.826
epoch 4,	batch   100,	training loss: 5.736
epoch 4,	batch   110,	training loss: 5.169
epoch 4,	batch   120,	training loss: 4.350
epoch 4,	batch   130,	training loss: 4.117
epoch 4,	batch   140,	training loss: 3.963
epoch 4,	batch   150,	training loss: 4.897
epoch 4,	batch   160,	training loss: 5.533
epoch 4,	batch   170,	training loss: 4.099
epoch 4,	batch   180,	training loss: 5.132
epoch 4,	batch   190,	training loss: 5.283
epoch 4,	batch   200,	training loss: 4.991
epoch 4,	batch   210,	training loss: 4.677
epoch 4,	batch   220,	training loss: 6.275
epoch 4,	batch   230,	training loss: 6.266
epoch 4,	batch   240,	training loss: 5.600
epoch 4,	batch   250,	training loss: 4.127
epoch 4,	batch   260,	training loss: 4.049
epoch 4,	batch   270,	training loss: 5.936
epoch 4,	batch   280,	training loss: 5.943
epoch 4,	batch   290,	training loss: 4.667
epoch 4,	batch   300,	training loss: 3.272
epoch 4,	batch   310,	training loss: 4.887
end of epoch 4
testing on validation set:
# correct:  10985/20000 = 54.925%
# off by 1: 5749/20000 = 28.745%
epoch 5,	batch    10,	training loss: 3.185
epoch 5,	batch    20,	training loss: 3.360
epoch 5,	batch    30,	training loss: 3.602
epoch 5,	batch    40,	training loss: 3.852
epoch 5,	batch    50,	training loss: 3.950
epoch 5,	batch    60,	training loss: 5.641
epoch 5,	batch    70,	training loss: 4.796
epoch 5,	batch    80,	training loss: 4.744
epoch 5,	batch    90,	training loss: 4.479
epoch 5,	batch   100,	training loss: 3.767
epoch 5,	batch   110,	training loss: 3.435
epoch 5,	batch   120,	training loss: 4.157
epoch 5,	batch   130,	training loss: 5.171
epoch 5,	batch   140,	training loss: 4.590
epoch 5,	batch   150,	training loss: 5.158
epoch 5,	batch   160,	training loss: 5.447
epoch 5,	batch   170,	training loss: 4.080
epoch 5,	batch   180,	training loss: 3.002
epoch 5,	batch   190,	training loss: 3.721
epoch 5,	batch   200,	training loss: 5.982
epoch 5,	batch   210,	training loss: 4.851
epoch 5,	batch   220,	training loss: 3.648
epoch 5,	batch   230,	training loss: 3.540
epoch 5,	batch   240,	training loss: 3.399
epoch 5,	batch   250,	training loss: 3.662
epoch 5,	batch   260,	training loss: 3.355
epoch 5,	batch   270,	training loss: 3.906
epoch 5,	batch   280,	training loss: 3.820
epoch 5,	batch   290,	training loss: 4.641
epoch 5,	batch   300,	training loss: 3.999
epoch 5,	batch   310,	training loss: 3.635
end of epoch 5
testing on validation set:
# correct:  9097/20000 = 45.485%
# off by 1: 6565/20000 = 32.825%
epoch 6,	batch    10,	training loss: 4.808
epoch 6,	batch    20,	training loss: 4.177
epoch 6,	batch    30,	training loss: 3.732
epoch 6,	batch    40,	training loss: 3.013
epoch 6,	batch    50,	training loss: 2.595
epoch 6,	batch    60,	training loss: 3.758
epoch 6,	batch    70,	training loss: 3.451
epoch 6,	batch    80,	training loss: 3.729
epoch 6,	batch    90,	training loss: 5.339
epoch 6,	batch   100,	training loss: 5.057
epoch 6,	batch   110,	training loss: 4.464
epoch 6,	batch   120,	training loss: 6.941
epoch 6,	batch   130,	training loss: 7.730
epoch 6,	batch   140,	training loss: 4.782
epoch 6,	batch   150,	training loss: 4.551
epoch 6,	batch   160,	training loss: 3.352
epoch 6,	batch   170,	training loss: 3.459
epoch 6,	batch   180,	training loss: 4.579
epoch 6,	batch   190,	training loss: 4.677
epoch 6,	batch   200,	training loss: 5.716
epoch 6,	batch   210,	training loss: 3.592
epoch 6,	batch   220,	training loss: 2.944
epoch 6,	batch   230,	training loss: 3.341
epoch 6,	batch   240,	training loss: 3.454
epoch 6,	batch   250,	training loss: 3.648
epoch 6,	batch   260,	training loss: 3.079
epoch 6,	batch   270,	training loss: 2.876
epoch 6,	batch   280,	training loss: 3.034
epoch 6,	batch   290,	training loss: 3.107
epoch 6,	batch   300,	training loss: 3.122
epoch 6,	batch   310,	training loss: 3.456
end of epoch 6
testing on validation set:
# correct:  10665/20000 = 53.325%
# off by 1: 5888/20000 = 29.44%
epoch 7,	batch    10,	training loss: 4.542
epoch 7,	batch    20,	training loss: 3.209
epoch 7,	batch    30,	training loss: 2.765
epoch 7,	batch    40,	training loss: 3.380
epoch 7,	batch    50,	training loss: 2.479
epoch 7,	batch    60,	training loss: 3.074
epoch 7,	batch    70,	training loss: 3.490
epoch 7,	batch    80,	training loss: 3.021
epoch 7,	batch    90,	training loss: 2.643
epoch 7,	batch   100,	training loss: 2.897
epoch 7,	batch   110,	training loss: 2.791
epoch 7,	batch   120,	training loss: 3.381
epoch 7,	batch   130,	training loss: 4.288
epoch 7,	batch   140,	training loss: 3.673
epoch 7,	batch   150,	training loss: 4.552
epoch 7,	batch   160,	training loss: 4.366
epoch 7,	batch   170,	training loss: 3.320
epoch 7,	batch   180,	training loss: 3.465
epoch 7,	batch   190,	training loss: 4.541
epoch 7,	batch   200,	training loss: 3.810
epoch 7,	batch   210,	training loss: 3.528
epoch 7,	batch   220,	training loss: 4.200
epoch 7,	batch   230,	training loss: 2.806
epoch 7,	batch   240,	training loss: 2.649
epoch 7,	batch   250,	training loss: 2.677
epoch 7,	batch   260,	training loss: 3.135
epoch 7,	batch   270,	training loss: 3.696
epoch 7,	batch   280,	training loss: 3.690
epoch 7,	batch   290,	training loss: 2.956
epoch 7,	batch   300,	training loss: 4.255
epoch 7,	batch   310,	training loss: 4.096
end of epoch 7
testing on validation set:
# correct:  10941/20000 = 54.705%
# off by 1: 5666/20000 = 28.33%
epoch 8,	batch    10,	training loss: 2.900
epoch 8,	batch    20,	training loss: 3.178
epoch 8,	batch    30,	training loss: 2.373
epoch 8,	batch    40,	training loss: 2.446
epoch 8,	batch    50,	training loss: 2.527
epoch 8,	batch    60,	training loss: 3.356
epoch 8,	batch    70,	training loss: 3.074
epoch 8,	batch    80,	training loss: 3.373
epoch 8,	batch    90,	training loss: 3.238
epoch 8,	batch   100,	training loss: 3.246
epoch 8,	batch   110,	training loss: 2.872
epoch 8,	batch   120,	training loss: 4.282
epoch 8,	batch   130,	training loss: 6.644
epoch 8,	batch   140,	training loss: 6.427
epoch 8,	batch   150,	training loss: 4.510
epoch 8,	batch   160,	training loss: 4.714
epoch 8,	batch   170,	training loss: 3.839
epoch 8,	batch   180,	training loss: 4.344
epoch 8,	batch   190,	training loss: 3.424
epoch 8,	batch   200,	training loss: 2.804
epoch 8,	batch   210,	training loss: 3.297
epoch 8,	batch   220,	training loss: 3.295
epoch 8,	batch   230,	training loss: 2.794
epoch 8,	batch   240,	training loss: 2.917
epoch 8,	batch   250,	training loss: 2.125
epoch 8,	batch   260,	training loss: 2.901
epoch 8,	batch   270,	training loss: 2.986
epoch 8,	batch   280,	training loss: 2.563
epoch 8,	batch   290,	training loss: 2.101
epoch 8,	batch   300,	training loss: 3.532
epoch 8,	batch   310,	training loss: 4.407
end of epoch 8
testing on validation set:
# correct:  10615/20000 = 53.075%
# off by 1: 5811/20000 = 29.055%
epoch 9,	batch    10,	training loss: 3.837
epoch 9,	batch    20,	training loss: 4.105
epoch 9,	batch    30,	training loss: 5.381
epoch 9,	batch    40,	training loss: 4.973
epoch 9,	batch    50,	training loss: 4.402
epoch 9,	batch    60,	training loss: 2.806
epoch 9,	batch    70,	training loss: 3.382
epoch 9,	batch    80,	training loss: 3.692
epoch 9,	batch    90,	training loss: 2.524
epoch 9,	batch   100,	training loss: 2.495
epoch 9,	batch   110,	training loss: 3.512
epoch 9,	batch   120,	training loss: 4.251
epoch 9,	batch   130,	training loss: 3.477
epoch 9,	batch   140,	training loss: 3.981
epoch 9,	batch   150,	training loss: 3.756
epoch 9,	batch   160,	training loss: 3.429
epoch 9,	batch   170,	training loss: 2.646
epoch 9,	batch   180,	training loss: 2.504
epoch 9,	batch   190,	training loss: 3.868
epoch 9,	batch   200,	training loss: 3.855
epoch 9,	batch   210,	training loss: 2.593
epoch 9,	batch   220,	training loss: 2.296
epoch 9,	batch   230,	training loss: 3.116
epoch 9,	batch   240,	training loss: 1.888
epoch 9,	batch   250,	training loss: 2.628
epoch 9,	batch   260,	training loss: 3.025
epoch 9,	batch   270,	training loss: 3.575
epoch 9,	batch   280,	training loss: 3.530
epoch 9,	batch   290,	training loss: 2.876
epoch 9,	batch   300,	training loss: 2.124
epoch 9,	batch   310,	training loss: 2.133
end of epoch 9
testing on validation set:
# correct:  11964/20000 = 59.82%
# off by 1: 5444/20000 = 27.22%
epoch 10,	batch    10,	training loss: 2.906
epoch 10,	batch    20,	training loss: 2.445
epoch 10,	batch    30,	training loss: 2.571
epoch 10,	batch    40,	training loss: 1.905
epoch 10,	batch    50,	training loss: 3.799
epoch 10,	batch    60,	training loss: 5.856
epoch 10,	batch    70,	training loss: 3.741
epoch 10,	batch    80,	training loss: 2.454
epoch 10,	batch    90,	training loss: 2.543
epoch 10,	batch   100,	training loss: 2.372
epoch 10,	batch   110,	training loss: 2.875
epoch 10,	batch   120,	training loss: 2.725
epoch 10,	batch   130,	training loss: 3.321
epoch 10,	batch   140,	training loss: 2.866
epoch 10,	batch   150,	training loss: 2.206
epoch 10,	batch   160,	training loss: 2.420
epoch 10,	batch   170,	training loss: 2.911
epoch 10,	batch   180,	training loss: 5.242
epoch 10,	batch   190,	training loss: 4.275
epoch 10,	batch   200,	training loss: 3.773
epoch 10,	batch   210,	training loss: 3.224
epoch 10,	batch   220,	training loss: 3.658
epoch 10,	batch   230,	training loss: 4.057
epoch 10,	batch   240,	training loss: 2.720
epoch 10,	batch   250,	training loss: 2.696
epoch 10,	batch   260,	training loss: 2.704
epoch 10,	batch   270,	training loss: 3.384
epoch 10,	batch   280,	training loss: 2.083
epoch 10,	batch   290,	training loss: 4.428
epoch 10,	batch   300,	training loss: 3.136
epoch 10,	batch   310,	training loss: 2.477
end of epoch 10
testing on validation set:
# correct:  8870/20000 = 44.35%
# off by 1: 5975/20000 = 29.875%
epoch 11,	batch    10,	training loss: 3.572
epoch 11,	batch    20,	training loss: 2.662
epoch 11,	batch    30,	training loss: 2.044
epoch 11,	batch    40,	training loss: 2.339
epoch 11,	batch    50,	training loss: 3.337
epoch 11,	batch    60,	training loss: 2.376
epoch 11,	batch    70,	training loss: 2.295
epoch 11,	batch    80,	training loss: 2.389
epoch 11,	batch    90,	training loss: 2.211
epoch 11,	batch   100,	training loss: 1.580
epoch 11,	batch   110,	training loss: 2.303
epoch 11,	batch   120,	training loss: 2.718
epoch 11,	batch   130,	training loss: 2.028
epoch 11,	batch   140,	training loss: 2.641
epoch 11,	batch   150,	training loss: 3.315
epoch 11,	batch   160,	training loss: 3.293
epoch 11,	batch   170,	training loss: 3.436
epoch 11,	batch   180,	training loss: 3.704
epoch 11,	batch   190,	training loss: 2.465
epoch 11,	batch   200,	training loss: 2.217
epoch 11,	batch   210,	training loss: 2.685
epoch 11,	batch   220,	training loss: 2.168
epoch 11,	batch   230,	training loss: 1.594
epoch 11,	batch   240,	training loss: 2.034
epoch 11,	batch   250,	training loss: 2.454
epoch 11,	batch   260,	training loss: 3.306
epoch 11,	batch   270,	training loss: 2.404
epoch 11,	batch   280,	training loss: 2.293
epoch 11,	batch   290,	training loss: 2.291
epoch 11,	batch   300,	training loss: 3.169
epoch 11,	batch   310,	training loss: 3.306
end of epoch 11
testing on validation set:
# correct:  12158/20000 = 60.79%
# off by 1: 5368/20000 = 26.84%
epoch 12,	batch    10,	training loss: 2.415
epoch 12,	batch    20,	training loss: 2.378
epoch 12,	batch    30,	training loss: 2.025
epoch 12,	batch    40,	training loss: 2.321
epoch 12,	batch    50,	training loss: 2.467
epoch 12,	batch    60,	training loss: 2.026
epoch 12,	batch    70,	training loss: 1.922
epoch 12,	batch    80,	training loss: 2.029
epoch 12,	batch    90,	training loss: 2.633
epoch 12,	batch   100,	training loss: 2.520
epoch 12,	batch   110,	training loss: 2.483
epoch 12,	batch   120,	training loss: 2.214
epoch 12,	batch   130,	training loss: 2.303
epoch 12,	batch   140,	training loss: 1.561
epoch 12,	batch   150,	training loss: 1.908
epoch 12,	batch   160,	training loss: 3.353
epoch 12,	batch   170,	training loss: 1.992
epoch 12,	batch   180,	training loss: 1.566
epoch 12,	batch   190,	training loss: 1.487
epoch 12,	batch   200,	training loss: 2.127
epoch 12,	batch   210,	training loss: 1.902
epoch 12,	batch   220,	training loss: 2.465
epoch 12,	batch   230,	training loss: 2.505
epoch 12,	batch   240,	training loss: 2.016
epoch 12,	batch   250,	training loss: 1.813
epoch 12,	batch   260,	training loss: 2.862
epoch 12,	batch   270,	training loss: 2.705
epoch 12,	batch   280,	training loss: 2.925
epoch 12,	batch   290,	training loss: 2.586
epoch 12,	batch   300,	training loss: 2.680
epoch 12,	batch   310,	training loss: 1.817
end of epoch 12
testing on validation set:
# correct:  6516/20000 = 32.58%
# off by 1: 6150/20000 = 30.75%
epoch 13,	batch    10,	training loss: 2.702
epoch 13,	batch    20,	training loss: 1.488
epoch 13,	batch    30,	training loss: 2.138
epoch 13,	batch    40,	training loss: 2.511
epoch 13,	batch    50,	training loss: 2.441
epoch 13,	batch    60,	training loss: 2.454
epoch 13,	batch    70,	training loss: 1.674
epoch 13,	batch    80,	training loss: 2.094
epoch 13,	batch    90,	training loss: 2.395
epoch 13,	batch   100,	training loss: 3.867
epoch 13,	batch   110,	training loss: 3.521
epoch 13,	batch   120,	training loss: 3.221
epoch 13,	batch   130,	training loss: 2.260
epoch 13,	batch   140,	training loss: 1.546
epoch 13,	batch   150,	training loss: 1.899
epoch 13,	batch   160,	training loss: 2.576
epoch 13,	batch   170,	training loss: 2.458
epoch 13,	batch   180,	training loss: 2.877
epoch 13,	batch   190,	training loss: 3.050
epoch 13,	batch   200,	training loss: 2.786
epoch 13,	batch   210,	training loss: 2.516
epoch 13,	batch   220,	training loss: 2.505
epoch 13,	batch   230,	training loss: 1.700
epoch 13,	batch   240,	training loss: 1.840
epoch 13,	batch   250,	training loss: 1.610
epoch 13,	batch   260,	training loss: 2.630
epoch 13,	batch   270,	training loss: 7.724
epoch 13,	batch   280,	training loss: 4.444
epoch 13,	batch   290,	training loss: 3.890
epoch 13,	batch   300,	training loss: 2.603
epoch 13,	batch   310,	training loss: 2.004
end of epoch 13
testing on validation set:
# correct:  10469/20000 = 52.345%
# off by 1: 6034/20000 = 30.17%
epoch 14,	batch    10,	training loss: 2.763
epoch 14,	batch    20,	training loss: 2.374
epoch 14,	batch    30,	training loss: 1.861
epoch 14,	batch    40,	training loss: 2.109
epoch 14,	batch    50,	training loss: 1.796
epoch 14,	batch    60,	training loss: 1.282
epoch 14,	batch    70,	training loss: 1.843
epoch 14,	batch    80,	training loss: 2.674
epoch 14,	batch    90,	training loss: 2.077
epoch 14,	batch   100,	training loss: 3.894
epoch 14,	batch   110,	training loss: 3.181
epoch 14,	batch   120,	training loss: 2.511
epoch 14,	batch   130,	training loss: 2.419
epoch 14,	batch   140,	training loss: 2.169
epoch 14,	batch   150,	training loss: 1.537
epoch 14,	batch   160,	training loss: 2.345
epoch 14,	batch   170,	training loss: 1.747
epoch 14,	batch   180,	training loss: 2.601
epoch 14,	batch   190,	training loss: 2.908
epoch 14,	batch   200,	training loss: 2.790
epoch 14,	batch   210,	training loss: 2.018
epoch 14,	batch   220,	training loss: 2.008
epoch 14,	batch   230,	training loss: 2.341
epoch 14,	batch   240,	training loss: 1.766
epoch 14,	batch   250,	training loss: 1.903
epoch 14,	batch   260,	training loss: 1.818
epoch 14,	batch   270,	training loss: 2.126
epoch 14,	batch   280,	training loss: 2.033
epoch 14,	batch   290,	training loss: 1.922
epoch 14,	batch   300,	training loss: 1.490
epoch 14,	batch   310,	training loss: 2.016
end of epoch 14
testing on validation set:
# correct:  8653/20000 = 43.265%
# off by 1: 5628/20000 = 28.14%
epoch 15,	batch    10,	training loss: 4.412
epoch 15,	batch    20,	training loss: 2.331
epoch 15,	batch    30,	training loss: 2.101
epoch 15,	batch    40,	training loss: 2.389
epoch 15,	batch    50,	training loss: 3.233
epoch 15,	batch    60,	training loss: 2.194
epoch 15,	batch    70,	training loss: 2.301
epoch 15,	batch    80,	training loss: 2.628
epoch 15,	batch    90,	training loss: 3.720
epoch 15,	batch   100,	training loss: 5.380
epoch 15,	batch   110,	training loss: 4.343
epoch 15,	batch   120,	training loss: 3.805
epoch 15,	batch   130,	training loss: 2.974
epoch 15,	batch   140,	training loss: 2.452
epoch 15,	batch   150,	training loss: 2.316
epoch 15,	batch   160,	training loss: 2.113
epoch 15,	batch   170,	training loss: 1.866
epoch 15,	batch   180,	training loss: 2.726
epoch 15,	batch   190,	training loss: 3.728
epoch 15,	batch   200,	training loss: 3.534
epoch 15,	batch   210,	training loss: 2.242
epoch 15,	batch   220,	training loss: 2.795
epoch 15,	batch   230,	training loss: 4.476
epoch 15,	batch   240,	training loss: 1.913
epoch 15,	batch   250,	training loss: 2.755
epoch 15,	batch   260,	training loss: 2.288
epoch 15,	batch   270,	training loss: 2.671
epoch 15,	batch   280,	training loss: 3.731
epoch 15,	batch   290,	training loss: 2.954
epoch 15,	batch   300,	training loss: 2.668
epoch 15,	batch   310,	training loss: 2.592
end of epoch 15
testing on validation set:
# correct:  9878/20000 = 49.39%
# off by 1: 6155/20000 = 30.775%
epoch 16,	batch    10,	training loss: 2.830
epoch 16,	batch    20,	training loss: 3.019
epoch 16,	batch    30,	training loss: 2.981
epoch 16,	batch    40,	training loss: 2.238
epoch 16,	batch    50,	training loss: 3.045
epoch 16,	batch    60,	training loss: 1.694
epoch 16,	batch    70,	training loss: 1.861
epoch 16,	batch    80,	training loss: 1.449
epoch 16,	batch    90,	training loss: 1.518
epoch 16,	batch   100,	training loss: 1.559
epoch 16,	batch   110,	training loss: 2.020
epoch 16,	batch   120,	training loss: 1.452
epoch 16,	batch   130,	training loss: 1.574
epoch 16,	batch   140,	training loss: 1.809
epoch 16,	batch   150,	training loss: 3.357
epoch 16,	batch   160,	training loss: 2.459
epoch 16,	batch   170,	training loss: 2.602
epoch 16,	batch   180,	training loss: 2.824
epoch 16,	batch   190,	training loss: 2.481
epoch 16,	batch   200,	training loss: 1.806
epoch 16,	batch   210,	training loss: 1.408
epoch 16,	batch   220,	training loss: 2.753
epoch 16,	batch   230,	training loss: 3.635
epoch 16,	batch   240,	training loss: 2.241
epoch 16,	batch   250,	training loss: 2.221
epoch 16,	batch   260,	training loss: 2.995
epoch 16,	batch   270,	training loss: 1.945
epoch 16,	batch   280,	training loss: 1.841
epoch 16,	batch   290,	training loss: 2.106
epoch 16,	batch   300,	training loss: 2.204
epoch 16,	batch   310,	training loss: 2.460
end of epoch 16
testing on validation set:
# correct:  11420/20000 = 57.1%
# off by 1: 5759/20000 = 28.795%
epoch 17,	batch    10,	training loss: 2.498
epoch 17,	batch    20,	training loss: 2.291
epoch 17,	batch    30,	training loss: 2.473
epoch 17,	batch    40,	training loss: 3.790
epoch 17,	batch    50,	training loss: 2.384
epoch 17,	batch    60,	training loss: 2.278
epoch 17,	batch    70,	training loss: 2.262
epoch 17,	batch    80,	training loss: 2.127
epoch 17,	batch    90,	training loss: 1.287
epoch 17,	batch   100,	training loss: 1.771
epoch 17,	batch   110,	training loss: 1.044
epoch 17,	batch   120,	training loss: 1.363
epoch 17,	batch   130,	training loss: 2.116
epoch 17,	batch   140,	training loss: 2.574
epoch 17,	batch   150,	training loss: 2.090
epoch 17,	batch   160,	training loss: 2.617
epoch 17,	batch   170,	training loss: 2.705
epoch 17,	batch   180,	training loss: 2.656
epoch 17,	batch   190,	training loss: 2.272
epoch 17,	batch   200,	training loss: 2.599
epoch 17,	batch   210,	training loss: 2.638
epoch 17,	batch   220,	training loss: 2.113
epoch 17,	batch   230,	training loss: 1.803
epoch 17,	batch   240,	training loss: 1.538
epoch 17,	batch   250,	training loss: 1.922
epoch 17,	batch   260,	training loss: 1.802
epoch 17,	batch   270,	training loss: 1.362
epoch 17,	batch   280,	training loss: 1.991
epoch 17,	batch   290,	training loss: 1.701
epoch 17,	batch   300,	training loss: 2.071
epoch 17,	batch   310,	training loss: 2.189
end of epoch 17
testing on validation set:
# correct:  9685/20000 = 48.425%
# off by 1: 6046/20000 = 30.23%
epoch 18,	batch    10,	training loss: 4.109
epoch 18,	batch    20,	training loss: 3.399
epoch 18,	batch    30,	training loss: 2.419
epoch 18,	batch    40,	training loss: 2.375
epoch 18,	batch    50,	training loss: 2.539
epoch 18,	batch    60,	training loss: 3.300
epoch 18,	batch    70,	training loss: 2.341
epoch 18,	batch    80,	training loss: 2.588
epoch 18,	batch    90,	training loss: 1.898
epoch 18,	batch   100,	training loss: 2.084
epoch 18,	batch   110,	training loss: 1.959
epoch 18,	batch   120,	training loss: 1.631
epoch 18,	batch   130,	training loss: 1.853
epoch 18,	batch   140,	training loss: 2.149
epoch 18,	batch   150,	training loss: 1.568
epoch 18,	batch   160,	training loss: 1.507
epoch 18,	batch   170,	training loss: 1.941
epoch 18,	batch   180,	training loss: 1.737
epoch 18,	batch   190,	training loss: 1.856
epoch 18,	batch   200,	training loss: 1.951
epoch 18,	batch   210,	training loss: 1.768
epoch 18,	batch   220,	training loss: 2.287
epoch 18,	batch   230,	training loss: 1.730
epoch 18,	batch   240,	training loss: 1.438
epoch 18,	batch   250,	training loss: 1.468
epoch 18,	batch   260,	training loss: 1.699
epoch 18,	batch   270,	training loss: 2.248
epoch 18,	batch   280,	training loss: 1.628
epoch 18,	batch   290,	training loss: 1.774
epoch 18,	batch   300,	training loss: 1.348
epoch 18,	batch   310,	training loss: 2.182
end of epoch 18
testing on validation set:
# correct:  11333/20000 = 56.665%
# off by 1: 5496/20000 = 27.48%
epoch 19,	batch    10,	training loss: 1.035
epoch 19,	batch    20,	training loss: 1.555
epoch 19,	batch    30,	training loss: 2.666
epoch 19,	batch    40,	training loss: 3.275
epoch 19,	batch    50,	training loss: 3.485
epoch 19,	batch    60,	training loss: 2.329
epoch 19,	batch    70,	training loss: 2.217
epoch 19,	batch    80,	training loss: 1.837
epoch 19,	batch    90,	training loss: 2.211
epoch 19,	batch   100,	training loss: 2.242
epoch 19,	batch   110,	training loss: 3.213
epoch 19,	batch   120,	training loss: 2.375
epoch 19,	batch   130,	training loss: 1.262
epoch 19,	batch   140,	training loss: 1.256
epoch 19,	batch   150,	training loss: 1.876
epoch 19,	batch   160,	training loss: 2.367
epoch 19,	batch   170,	training loss: 1.890
epoch 19,	batch   180,	training loss: 1.737
epoch 19,	batch   190,	training loss: 1.308
epoch 19,	batch   200,	training loss: 1.507
epoch 19,	batch   210,	training loss: 1.945
epoch 19,	batch   220,	training loss: 2.423
epoch 19,	batch   230,	training loss: 1.750
epoch 19,	batch   240,	training loss: 1.589
epoch 19,	batch   250,	training loss: 1.850
epoch 19,	batch   260,	training loss: 1.766
epoch 19,	batch   270,	training loss: 1.740
epoch 19,	batch   280,	training loss: 1.644
epoch 19,	batch   290,	training loss: 1.965
epoch 19,	batch   300,	training loss: 1.430
epoch 19,	batch   310,	training loss: 1.958
end of epoch 19
testing on validation set:
# correct:  7631/20000 = 38.155%
# off by 1: 6781/20000 = 33.905%
epoch 20,	batch    10,	training loss: 2.185
epoch 20,	batch    20,	training loss: 2.088
epoch 20,	batch    30,	training loss: 1.047
epoch 20,	batch    40,	training loss: 1.419
epoch 20,	batch    50,	training loss: 1.476
epoch 20,	batch    60,	training loss: 1.752
epoch 20,	batch    70,	training loss: 1.371
epoch 20,	batch    80,	training loss: 1.474
epoch 20,	batch    90,	training loss: 1.676
epoch 20,	batch   100,	training loss: 2.248
epoch 20,	batch   110,	training loss: 1.930
epoch 20,	batch   120,	training loss: 1.511
epoch 20,	batch   130,	training loss: 1.696
epoch 20,	batch   140,	training loss: 2.104
epoch 20,	batch   150,	training loss: 1.402
epoch 20,	batch   160,	training loss: 1.592
epoch 20,	batch   170,	training loss: 1.384
epoch 20,	batch   180,	training loss: 1.588
epoch 20,	batch   190,	training loss: 1.311
epoch 20,	batch   200,	training loss: 1.350
epoch 20,	batch   210,	training loss: 1.169
epoch 20,	batch   220,	training loss: 1.520
epoch 20,	batch   230,	training loss: 1.846
epoch 20,	batch   240,	training loss: 1.375
epoch 20,	batch   250,	training loss: 1.608
epoch 20,	batch   260,	training loss: 2.559
epoch 20,	batch   270,	training loss: 2.209
epoch 20,	batch   280,	training loss: 1.727
epoch 20,	batch   290,	training loss: 2.228
epoch 20,	batch   300,	training loss: 3.186
epoch 20,	batch   310,	training loss: 2.448
end of epoch 20
testing on validation set:
# correct:  11031/20000 = 55.155%
# off by 1: 5654/20000 = 28.27%
epoch 21,	batch    10,	training loss: 1.727
epoch 21,	batch    20,	training loss: 2.494
epoch 21,	batch    30,	training loss: 2.686
epoch 21,	batch    40,	training loss: 3.374
epoch 21,	batch    50,	training loss: 3.173
epoch 21,	batch    60,	training loss: 2.578
epoch 21,	batch    70,	training loss: 2.680
epoch 21,	batch    80,	training loss: 2.724
epoch 21,	batch    90,	training loss: 1.758
epoch 21,	batch   100,	training loss: 1.215
epoch 21,	batch   110,	training loss: 1.392
epoch 21,	batch   120,	training loss: 1.351
epoch 21,	batch   130,	training loss: 2.197
epoch 21,	batch   140,	training loss: 3.153
epoch 21,	batch   150,	training loss: 1.759
epoch 21,	batch   160,	training loss: 1.634
epoch 21,	batch   170,	training loss: 1.618
epoch 21,	batch   180,	training loss: 2.550
epoch 21,	batch   190,	training loss: 3.424
epoch 21,	batch   200,	training loss: 4.177
epoch 21,	batch   210,	training loss: 2.531
epoch 21,	batch   220,	training loss: 1.998
epoch 21,	batch   230,	training loss: 1.902
epoch 21,	batch   240,	training loss: 1.663
epoch 21,	batch   250,	training loss: 1.960
epoch 21,	batch   260,	training loss: 2.709
epoch 21,	batch   270,	training loss: 2.331
epoch 21,	batch   280,	training loss: 2.209
epoch 21,	batch   290,	training loss: 3.002
epoch 21,	batch   300,	training loss: 2.702
epoch 21,	batch   310,	training loss: 3.742
end of epoch 21
testing on validation set:
# correct:  10498/20000 = 52.49%
# off by 1: 5935/20000 = 29.675%
epoch 22,	batch    10,	training loss: 1.955
epoch 22,	batch    20,	training loss: 2.658
epoch 22,	batch    30,	training loss: 1.661
epoch 22,	batch    40,	training loss: 1.705
epoch 22,	batch    50,	training loss: 1.794
epoch 22,	batch    60,	training loss: 1.592
epoch 22,	batch    70,	training loss: 2.348
epoch 22,	batch    80,	training loss: 2.593
epoch 22,	batch    90,	training loss: 2.035
epoch 22,	batch   100,	training loss: 1.531
epoch 22,	batch   110,	training loss: 2.306
epoch 22,	batch   120,	training loss: 1.362
epoch 22,	batch   130,	training loss: 1.644
epoch 22,	batch   140,	training loss: 1.252
epoch 22,	batch   150,	training loss: 1.442
epoch 22,	batch   160,	training loss: 1.654
epoch 22,	batch   170,	training loss: 1.571
epoch 22,	batch   180,	training loss: 1.666
epoch 22,	batch   190,	training loss: 1.717
epoch 22,	batch   200,	training loss: 2.342
epoch 22,	batch   210,	training loss: 3.239
epoch 22,	batch   220,	training loss: 4.833
epoch 22,	batch   230,	training loss: 4.176
epoch 22,	batch   240,	training loss: 3.610
epoch 22,	batch   250,	training loss: 2.392
epoch 22,	batch   260,	training loss: 3.103
epoch 22,	batch   270,	training loss: 2.183
epoch 22,	batch   280,	training loss: 3.395
epoch 22,	batch   290,	training loss: 2.345
epoch 22,	batch   300,	training loss: 1.983
epoch 22,	batch   310,	training loss: 1.779
end of epoch 22
testing on validation set:
# correct:  11971/20000 = 59.855%
# off by 1: 5479/20000 = 27.395%
epoch 23,	batch    10,	training loss: 1.240
epoch 23,	batch    20,	training loss: 1.097
epoch 23,	batch    30,	training loss: 1.260
epoch 23,	batch    40,	training loss: 1.764
epoch 23,	batch    50,	training loss: 1.742
epoch 23,	batch    60,	training loss: 2.052
epoch 23,	batch    70,	training loss: 2.102
epoch 23,	batch    80,	training loss: 1.865
epoch 23,	batch    90,	training loss: 1.784
epoch 23,	batch   100,	training loss: 2.952
epoch 23,	batch   110,	training loss: 3.268
epoch 23,	batch   120,	training loss: 2.791
epoch 23,	batch   130,	training loss: 2.326
epoch 23,	batch   140,	training loss: 2.310
epoch 23,	batch   150,	training loss: 1.899
epoch 23,	batch   160,	training loss: 1.830
epoch 23,	batch   170,	training loss: 1.427
epoch 23,	batch   180,	training loss: 1.435
epoch 23,	batch   190,	training loss: 1.401
epoch 23,	batch   200,	training loss: 1.128
epoch 23,	batch   210,	training loss: 1.654
epoch 23,	batch   220,	training loss: 1.617
epoch 23,	batch   230,	training loss: 1.639
epoch 23,	batch   240,	training loss: 1.367
epoch 23,	batch   250,	training loss: 1.834
epoch 23,	batch   260,	training loss: 1.927
epoch 23,	batch   270,	training loss: 1.516
epoch 23,	batch   280,	training loss: 1.467
epoch 23,	batch   290,	training loss: 2.330
epoch 23,	batch   300,	training loss: 3.289
epoch 23,	batch   310,	training loss: 2.875
end of epoch 23
testing on validation set:
# correct:  9318/20000 = 46.59%
# off by 1: 5946/20000 = 29.73%
epoch 24,	batch    10,	training loss: 2.193
epoch 24,	batch    20,	training loss: 2.737
epoch 24,	batch    30,	training loss: 1.517
epoch 24,	batch    40,	training loss: 1.251
epoch 24,	batch    50,	training loss: 1.337
epoch 24,	batch    60,	training loss: 1.752
epoch 24,	batch    70,	training loss: 1.309
epoch 24,	batch    80,	training loss: 2.003
epoch 24,	batch    90,	training loss: 1.562
epoch 24,	batch   100,	training loss: 1.348
epoch 24,	batch   110,	training loss: 1.267
epoch 24,	batch   120,	training loss: 1.773
epoch 24,	batch   130,	training loss: 1.672
epoch 24,	batch   140,	training loss: 1.358
epoch 24,	batch   150,	training loss: 1.222
epoch 24,	batch   160,	training loss: 1.715
epoch 24,	batch   170,	training loss: 3.015
epoch 24,	batch   180,	training loss: 2.568
epoch 24,	batch   190,	training loss: 2.392
epoch 24,	batch   200,	training loss: 2.348
epoch 24,	batch   210,	training loss: 2.567
epoch 24,	batch   220,	training loss: 2.652
epoch 24,	batch   230,	training loss: 2.418
epoch 24,	batch   240,	training loss: 2.386
epoch 24,	batch   250,	training loss: 3.068
epoch 24,	batch   260,	training loss: 2.204
epoch 24,	batch   270,	training loss: 2.683
epoch 24,	batch   280,	training loss: 2.564
epoch 24,	batch   290,	training loss: 2.718
epoch 24,	batch   300,	training loss: 2.088
epoch 24,	batch   310,	training loss: 1.781
end of epoch 24
testing on validation set:
# correct:  10206/20000 = 51.03%
# off by 1: 5894/20000 = 29.47%
epoch 25,	batch    10,	training loss: 1.973
epoch 25,	batch    20,	training loss: 1.385
epoch 25,	batch    30,	training loss: 1.385
epoch 25,	batch    40,	training loss: 1.294
epoch 25,	batch    50,	training loss: 1.322
epoch 25,	batch    60,	training loss: 1.395
epoch 25,	batch    70,	training loss: 2.112
epoch 25,	batch    80,	training loss: 2.269
epoch 25,	batch    90,	training loss: 1.640
epoch 25,	batch   100,	training loss: 1.049
epoch 25,	batch   110,	training loss: 1.334
epoch 25,	batch   120,	training loss: 1.630
epoch 25,	batch   130,	training loss: 1.682
epoch 25,	batch   140,	training loss: 1.284
epoch 25,	batch   150,	training loss: 0.960
epoch 25,	batch   160,	training loss: 1.308
epoch 25,	batch   170,	training loss: 1.364
epoch 25,	batch   180,	training loss: 1.867
epoch 25,	batch   190,	training loss: 1.917
epoch 25,	batch   200,	training loss: 2.824
epoch 25,	batch   210,	training loss: 2.436
epoch 25,	batch   220,	training loss: 2.198
epoch 25,	batch   230,	training loss: 2.009
epoch 25,	batch   240,	training loss: 2.076
epoch 25,	batch   250,	training loss: 1.399
epoch 25,	batch   260,	training loss: 1.882
epoch 25,	batch   270,	training loss: 1.910
epoch 25,	batch   280,	training loss: 2.377
epoch 25,	batch   290,	training loss: 1.761
epoch 25,	batch   300,	training loss: 1.326
epoch 25,	batch   310,	training loss: 1.348
end of epoch 25
testing on validation set:
# correct:  11537/20000 = 57.685%
# off by 1: 5549/20000 = 27.745%
epoch 26,	batch    10,	training loss: 1.212
epoch 26,	batch    20,	training loss: 1.979
epoch 26,	batch    30,	training loss: 1.866
epoch 26,	batch    40,	training loss: 2.030
epoch 26,	batch    50,	training loss: 1.559
epoch 26,	batch    60,	training loss: 1.432
epoch 26,	batch    70,	training loss: 1.809
epoch 26,	batch    80,	training loss: 1.272
epoch 26,	batch    90,	training loss: 1.343
epoch 26,	batch   100,	training loss: 1.556
epoch 26,	batch   110,	training loss: 1.676
epoch 26,	batch   120,	training loss: 2.211
epoch 26,	batch   130,	training loss: 2.102
epoch 26,	batch   140,	training loss: 1.530
epoch 26,	batch   150,	training loss: 1.185
epoch 26,	batch   160,	training loss: 1.877
epoch 26,	batch   170,	training loss: 1.975
epoch 26,	batch   180,	training loss: 1.480
epoch 26,	batch   190,	training loss: 1.354
epoch 26,	batch   200,	training loss: 1.512
epoch 26,	batch   210,	training loss: 1.333
epoch 26,	batch   220,	training loss: 1.171
epoch 26,	batch   230,	training loss: 1.209
epoch 26,	batch   240,	training loss: 1.535
epoch 26,	batch   250,	training loss: 2.033
epoch 26,	batch   260,	training loss: 1.564
epoch 26,	batch   270,	training loss: 1.501
epoch 26,	batch   280,	training loss: 2.296
epoch 26,	batch   290,	training loss: 1.622
epoch 26,	batch   300,	training loss: 1.452
epoch 26,	batch   310,	training loss: 1.186
end of epoch 26
testing on validation set:
# correct:  12804/20000 = 64.02%
# off by 1: 4911/20000 = 24.555%
epoch 27,	batch    10,	training loss: 0.829
epoch 27,	batch    20,	training loss: 1.389
epoch 27,	batch    30,	training loss: 1.771
epoch 27,	batch    40,	training loss: 2.421
epoch 27,	batch    50,	training loss: 2.193
epoch 27,	batch    60,	training loss: 2.561
epoch 27,	batch    70,	training loss: 1.559
epoch 27,	batch    80,	training loss: 1.202
epoch 27,	batch    90,	training loss: 1.307
epoch 27,	batch   100,	training loss: 0.911
epoch 27,	batch   110,	training loss: 1.579
epoch 27,	batch   120,	training loss: 1.552
epoch 27,	batch   130,	training loss: 1.346
epoch 27,	batch   140,	training loss: 1.274
epoch 27,	batch   150,	training loss: 1.088
epoch 27,	batch   160,	training loss: 1.031
epoch 27,	batch   170,	training loss: 2.618
epoch 27,	batch   180,	training loss: 2.854
epoch 27,	batch   190,	training loss: 3.044
epoch 27,	batch   200,	training loss: 2.381
epoch 27,	batch   210,	training loss: 1.851
epoch 27,	batch   220,	training loss: 1.316
epoch 27,	batch   230,	training loss: 1.169
epoch 27,	batch   240,	training loss: 1.115
epoch 27,	batch   250,	training loss: 1.086
epoch 27,	batch   260,	training loss: 1.806
epoch 27,	batch   270,	training loss: 1.211
epoch 27,	batch   280,	training loss: 1.855
epoch 27,	batch   290,	training loss: 3.338
epoch 27,	batch   300,	training loss: 2.466
epoch 27,	batch   310,	training loss: 1.746
end of epoch 27
testing on validation set:
# correct:  10844/20000 = 54.22%
# off by 1: 5781/20000 = 28.905%
epoch 28,	batch    10,	training loss: 1.397
epoch 28,	batch    20,	training loss: 1.594
epoch 28,	batch    30,	training loss: 1.251
epoch 28,	batch    40,	training loss: 1.451
epoch 28,	batch    50,	training loss: 1.462
epoch 28,	batch    60,	training loss: 1.791
epoch 28,	batch    70,	training loss: 2.329
epoch 28,	batch    80,	training loss: 1.544
epoch 28,	batch    90,	training loss: 1.322
epoch 28,	batch   100,	training loss: 1.283
epoch 28,	batch   110,	training loss: 1.265
epoch 28,	batch   120,	training loss: 1.252
epoch 28,	batch   130,	training loss: 0.835
epoch 28,	batch   140,	training loss: 1.526
epoch 28,	batch   150,	training loss: 2.692
epoch 28,	batch   160,	training loss: 2.249
epoch 28,	batch   170,	training loss: 2.517
epoch 28,	batch   180,	training loss: 2.056
epoch 28,	batch   190,	training loss: 1.658
epoch 28,	batch   200,	training loss: 1.428
epoch 28,	batch   210,	training loss: 1.242
epoch 28,	batch   220,	training loss: 1.448
epoch 28,	batch   230,	training loss: 1.561
epoch 28,	batch   240,	training loss: 1.135
epoch 28,	batch   250,	training loss: 0.880
epoch 28,	batch   260,	training loss: 1.124
epoch 28,	batch   270,	training loss: 1.820
epoch 28,	batch   280,	training loss: 1.692
epoch 28,	batch   290,	training loss: 1.755
epoch 28,	batch   300,	training loss: 2.194
epoch 28,	batch   310,	training loss: 1.436
end of epoch 28
testing on validation set:
# correct:  10356/20000 = 51.78%
# off by 1: 5983/20000 = 29.915%
epoch 29,	batch    10,	training loss: 1.575
epoch 29,	batch    20,	training loss: 1.726
epoch 29,	batch    30,	training loss: 1.928
epoch 29,	batch    40,	training loss: 1.422
epoch 29,	batch    50,	training loss: 0.998
epoch 29,	batch    60,	training loss: 1.600
epoch 29,	batch    70,	training loss: 1.911
epoch 29,	batch    80,	training loss: 2.648
epoch 29,	batch    90,	training loss: 1.946
epoch 29,	batch   100,	training loss: 1.474
epoch 29,	batch   110,	training loss: 1.212
epoch 29,	batch   120,	training loss: 1.152
epoch 29,	batch   130,	training loss: 1.500
epoch 29,	batch   140,	training loss: 1.591
epoch 29,	batch   150,	training loss: 1.134
epoch 29,	batch   160,	training loss: 1.183
epoch 29,	batch   170,	training loss: 1.207
epoch 29,	batch   180,	training loss: 1.174
epoch 29,	batch   190,	training loss: 1.002
epoch 29,	batch   200,	training loss: 1.212
epoch 29,	batch   210,	training loss: 1.167
epoch 29,	batch   220,	training loss: 0.863
epoch 29,	batch   230,	training loss: 1.029
epoch 29,	batch   240,	training loss: 1.309
epoch 29,	batch   250,	training loss: 1.101
epoch 29,	batch   260,	training loss: 1.521
epoch 29,	batch   270,	training loss: 1.297
epoch 29,	batch   280,	training loss: 0.865
epoch 29,	batch   290,	training loss: 1.556
epoch 29,	batch   300,	training loss: 0.860
epoch 29,	batch   310,	training loss: 1.064
end of epoch 29
testing on validation set:
# correct:  12054/20000 = 60.27%
# off by 1: 5293/20000 = 26.465%
epoch 30,	batch    10,	training loss: 1.267
epoch 30,	batch    20,	training loss: 0.929
epoch 30,	batch    30,	training loss: 1.313
epoch 30,	batch    40,	training loss: 0.957
epoch 30,	batch    50,	training loss: 1.454
epoch 30,	batch    60,	training loss: 1.287
epoch 30,	batch    70,	training loss: 1.065
epoch 30,	batch    80,	training loss: 0.957
epoch 30,	batch    90,	training loss: 1.321
epoch 30,	batch   100,	training loss: 1.489
epoch 30,	batch   110,	training loss: 0.994
epoch 30,	batch   120,	training loss: 1.469
epoch 30,	batch   130,	training loss: 1.560
epoch 30,	batch   140,	training loss: 1.144
epoch 30,	batch   150,	training loss: 1.239
epoch 30,	batch   160,	training loss: 1.390
epoch 30,	batch   170,	training loss: 1.756
epoch 30,	batch   180,	training loss: 1.681
epoch 30,	batch   190,	training loss: 1.710
epoch 30,	batch   200,	training loss: 1.163
epoch 30,	batch   210,	training loss: 1.308
epoch 30,	batch   220,	training loss: 1.212
epoch 30,	batch   230,	training loss: 1.703
epoch 30,	batch   240,	training loss: 1.404
epoch 30,	batch   250,	training loss: 1.662
epoch 30,	batch   260,	training loss: 2.738
epoch 30,	batch   270,	training loss: 3.289
epoch 30,	batch   280,	training loss: 1.852
epoch 30,	batch   290,	training loss: 1.802
epoch 30,	batch   300,	training loss: 1.722
epoch 30,	batch   310,	training loss: 1.402
end of epoch 30
testing on validation set:
# correct:  10533/20000 = 52.665%
# off by 1: 5934/20000 = 29.67%
epoch 31,	batch    10,	training loss: 1.531
epoch 31,	batch    20,	training loss: 2.025
epoch 31,	batch    30,	training loss: 1.990
epoch 31,	batch    40,	training loss: 1.217
epoch 31,	batch    50,	training loss: 0.794
epoch 31,	batch    60,	training loss: 0.991
epoch 31,	batch    70,	training loss: 1.109
epoch 31,	batch    80,	training loss: 1.155
epoch 31,	batch    90,	training loss: 0.867
epoch 31,	batch   100,	training loss: 0.831
epoch 31,	batch   110,	training loss: 0.978
epoch 31,	batch   120,	training loss: 0.914
epoch 31,	batch   130,	training loss: 0.936
epoch 31,	batch   140,	training loss: 0.908
epoch 31,	batch   150,	training loss: 1.634
epoch 31,	batch   160,	training loss: 1.994
epoch 31,	batch   170,	training loss: 1.288
epoch 31,	batch   180,	training loss: 1.622
epoch 31,	batch   190,	training loss: 1.889
epoch 31,	batch   200,	training loss: 1.461
epoch 31,	batch   210,	training loss: 0.988
epoch 31,	batch   220,	training loss: 0.779
epoch 31,	batch   230,	training loss: 1.122
epoch 31,	batch   240,	training loss: 1.530
epoch 31,	batch   250,	training loss: 1.346
epoch 31,	batch   260,	training loss: 1.205
epoch 31,	batch   270,	training loss: 1.199
epoch 31,	batch   280,	training loss: 1.052
epoch 31,	batch   290,	training loss: 1.099
epoch 31,	batch   300,	training loss: 1.069
epoch 31,	batch   310,	training loss: 1.226
end of epoch 31
testing on validation set:
# correct:  11059/20000 = 55.295%
# off by 1: 5599/20000 = 27.995%
epoch 32,	batch    10,	training loss: 1.077
epoch 32,	batch    20,	training loss: 1.046
epoch 32,	batch    30,	training loss: 0.995
epoch 32,	batch    40,	training loss: 1.380
epoch 32,	batch    50,	training loss: 1.450
epoch 32,	batch    60,	training loss: 1.034
epoch 32,	batch    70,	training loss: 1.029
epoch 32,	batch    80,	training loss: 1.218
epoch 32,	batch    90,	training loss: 1.017
epoch 32,	batch   100,	training loss: 1.393
epoch 32,	batch   110,	training loss: 1.464
epoch 32,	batch   120,	training loss: 1.045
epoch 32,	batch   130,	training loss: 0.742
epoch 32,	batch   140,	training loss: 1.119
epoch 32,	batch   150,	training loss: 1.017
epoch 32,	batch   160,	training loss: 1.345
epoch 32,	batch   170,	training loss: 1.159
epoch 32,	batch   180,	training loss: 1.171
epoch 32,	batch   190,	training loss: 0.906
epoch 32,	batch   200,	training loss: 1.030
epoch 32,	batch   210,	training loss: 1.214
epoch 32,	batch   220,	training loss: 1.063
epoch 32,	batch   230,	training loss: 1.104
epoch 32,	batch   240,	training loss: 1.289
epoch 32,	batch   250,	training loss: 2.331
epoch 32,	batch   260,	training loss: 2.542
epoch 32,	batch   270,	training loss: 1.422
epoch 32,	batch   280,	training loss: 1.524
epoch 32,	batch   290,	training loss: 1.148
epoch 32,	batch   300,	training loss: 0.952
epoch 32,	batch   310,	training loss: 1.161
end of epoch 32
testing on validation set:
# correct:  9550/20000 = 47.75%
# off by 1: 6342/20000 = 31.71%
epoch 33,	batch    10,	training loss: 1.708
epoch 33,	batch    20,	training loss: 1.842
epoch 33,	batch    30,	training loss: 1.344
epoch 33,	batch    40,	training loss: 1.180
epoch 33,	batch    50,	training loss: 1.034
epoch 33,	batch    60,	training loss: 0.787
epoch 33,	batch    70,	training loss: 0.881
epoch 33,	batch    80,	training loss: 1.445
epoch 33,	batch    90,	training loss: 1.285
epoch 33,	batch   100,	training loss: 1.486
epoch 33,	batch   110,	training loss: 1.015
epoch 33,	batch   120,	training loss: 1.044
epoch 33,	batch   130,	training loss: 0.987
epoch 33,	batch   140,	training loss: 1.131
epoch 33,	batch   150,	training loss: 0.910
epoch 33,	batch   160,	training loss: 0.844
epoch 33,	batch   170,	training loss: 1.175
epoch 33,	batch   180,	training loss: 1.404
epoch 33,	batch   190,	training loss: 0.932
epoch 33,	batch   200,	training loss: 1.006
epoch 33,	batch   210,	training loss: 0.979
epoch 33,	batch   220,	training loss: 1.020
epoch 33,	batch   230,	training loss: 1.715
epoch 33,	batch   240,	training loss: 1.348
epoch 33,	batch   250,	training loss: 1.328
epoch 33,	batch   260,	training loss: 1.407
epoch 33,	batch   270,	training loss: 1.223
epoch 33,	batch   280,	training loss: 1.242
epoch 33,	batch   290,	training loss: 1.175
epoch 33,	batch   300,	training loss: 1.352
epoch 33,	batch   310,	training loss: 1.196
end of epoch 33
testing on validation set:
# correct:  12514/20000 = 62.57%
# off by 1: 5070/20000 = 25.35%
epoch 34,	batch    10,	training loss: 1.505
epoch 34,	batch    20,	training loss: 1.265
epoch 34,	batch    30,	training loss: 1.239
epoch 34,	batch    40,	training loss: 1.184
epoch 34,	batch    50,	training loss: 0.948
epoch 34,	batch    60,	training loss: 1.061
epoch 34,	batch    70,	training loss: 1.121
epoch 34,	batch    80,	training loss: 3.869
epoch 34,	batch    90,	training loss: 2.575
epoch 34,	batch   100,	training loss: 1.716
epoch 34,	batch   110,	training loss: 1.532
epoch 34,	batch   120,	training loss: 1.541
epoch 34,	batch   130,	training loss: 1.011
epoch 34,	batch   140,	training loss: 0.942
epoch 34,	batch   150,	training loss: 1.055
epoch 34,	batch   160,	training loss: 0.883
epoch 34,	batch   170,	training loss: 1.134
epoch 34,	batch   180,	training loss: 1.055
epoch 34,	batch   190,	training loss: 1.469
epoch 34,	batch   200,	training loss: 1.577
epoch 34,	batch   210,	training loss: 1.246
epoch 34,	batch   220,	training loss: 1.171
epoch 34,	batch   230,	training loss: 1.943
epoch 34,	batch   240,	training loss: 1.225
epoch 34,	batch   250,	training loss: 0.974
epoch 34,	batch   260,	training loss: 1.420
epoch 34,	batch   270,	training loss: 1.091
epoch 34,	batch   280,	training loss: 1.309
epoch 34,	batch   290,	training loss: 1.009
epoch 34,	batch   300,	training loss: 1.214
epoch 34,	batch   310,	training loss: 1.267
end of epoch 34
testing on validation set:
# correct:  10713/20000 = 53.565%
# off by 1: 5683/20000 = 28.415%
epoch 35,	batch    10,	training loss: 2.272
epoch 35,	batch    20,	training loss: 1.478
epoch 35,	batch    30,	training loss: 1.271
epoch 35,	batch    40,	training loss: 1.318
epoch 35,	batch    50,	training loss: 1.359
epoch 35,	batch    60,	training loss: 1.480
epoch 35,	batch    70,	training loss: 1.683
epoch 35,	batch    80,	training loss: 1.842
epoch 35,	batch    90,	training loss: 1.526
epoch 35,	batch   100,	training loss: 1.190
epoch 35,	batch   110,	training loss: 1.299
epoch 35,	batch   120,	training loss: 1.211
epoch 35,	batch   130,	training loss: 0.921
epoch 35,	batch   140,	training loss: 1.139
epoch 35,	batch   150,	training loss: 1.580
epoch 35,	batch   160,	training loss: 1.005
epoch 35,	batch   170,	training loss: 1.126
epoch 35,	batch   180,	training loss: 0.806
epoch 35,	batch   190,	training loss: 1.035
epoch 35,	batch   200,	training loss: 0.830
epoch 35,	batch   210,	training loss: 0.838
epoch 35,	batch   220,	training loss: 0.989
epoch 35,	batch   230,	training loss: 1.094
epoch 35,	batch   240,	training loss: 0.995
epoch 35,	batch   250,	training loss: 0.919
epoch 35,	batch   260,	training loss: 1.088
epoch 35,	batch   270,	training loss: 0.904
epoch 35,	batch   280,	training loss: 0.923
epoch 35,	batch   290,	training loss: 1.285
epoch 35,	batch   300,	training loss: 1.301
epoch 35,	batch   310,	training loss: 1.032
end of epoch 35
testing on validation set:
# correct:  12061/20000 = 60.305%
# off by 1: 5129/20000 = 25.645%
epoch 36,	batch    10,	training loss: 1.146
epoch 36,	batch    20,	training loss: 1.008
epoch 36,	batch    30,	training loss: 1.021
epoch 36,	batch    40,	training loss: 1.063
epoch 36,	batch    50,	training loss: 0.903
epoch 36,	batch    60,	training loss: 1.348
epoch 36,	batch    70,	training loss: 2.258
epoch 36,	batch    80,	training loss: 1.934
epoch 36,	batch    90,	training loss: 1.667
epoch 36,	batch   100,	training loss: 1.230
epoch 36,	batch   110,	training loss: 1.023
epoch 36,	batch   120,	training loss: 0.792
epoch 36,	batch   130,	training loss: 1.010
epoch 36,	batch   140,	training loss: 0.892
epoch 36,	batch   150,	training loss: 0.740
epoch 36,	batch   160,	training loss: 0.976
epoch 36,	batch   170,	training loss: 1.445
epoch 36,	batch   180,	training loss: 1.331
epoch 36,	batch   190,	training loss: 2.162
epoch 36,	batch   200,	training loss: 1.276
epoch 36,	batch   210,	training loss: 1.391
epoch 36,	batch   220,	training loss: 1.337
epoch 36,	batch   230,	training loss: 2.306
epoch 36,	batch   240,	training loss: 1.619
epoch 36,	batch   250,	training loss: 1.251
epoch 36,	batch   260,	training loss: 1.047
epoch 36,	batch   270,	training loss: 1.044
epoch 36,	batch   280,	training loss: 0.953
epoch 36,	batch   290,	training loss: 1.282
epoch 36,	batch   300,	training loss: 1.318
epoch 36,	batch   310,	training loss: 1.037
end of epoch 36
testing on validation set:
# correct:  11419/20000 = 57.095%
# off by 1: 5551/20000 = 27.755%
epoch 37,	batch    10,	training loss: 0.965
epoch 37,	batch    20,	training loss: 0.784
epoch 37,	batch    30,	training loss: 0.711
epoch 37,	batch    40,	training loss: 0.879
epoch 37,	batch    50,	training loss: 0.691
epoch 37,	batch    60,	training loss: 0.844
epoch 37,	batch    70,	training loss: 1.179
epoch 37,	batch    80,	training loss: 1.368
epoch 37,	batch    90,	training loss: 0.722
epoch 37,	batch   100,	training loss: 0.730
epoch 37,	batch   110,	training loss: 0.759
epoch 37,	batch   120,	training loss: 0.884
epoch 37,	batch   130,	training loss: 1.600
epoch 37,	batch   140,	training loss: 0.859
epoch 37,	batch   150,	training loss: 0.771
epoch 37,	batch   160,	training loss: 0.835
epoch 37,	batch   170,	training loss: 0.805
epoch 37,	batch   180,	training loss: 0.812
epoch 37,	batch   190,	training loss: 0.637
epoch 37,	batch   200,	training loss: 0.671
epoch 37,	batch   210,	training loss: 1.058
epoch 37,	batch   220,	training loss: 1.005
epoch 37,	batch   230,	training loss: 0.963
epoch 37,	batch   240,	training loss: 0.883
epoch 37,	batch   250,	training loss: 0.950
epoch 37,	batch   260,	training loss: 1.570
epoch 37,	batch   270,	training loss: 1.099
epoch 37,	batch   280,	training loss: 0.759
epoch 37,	batch   290,	training loss: 0.975
epoch 37,	batch   300,	training loss: 1.585
epoch 37,	batch   310,	training loss: 1.315
end of epoch 37
testing on validation set:
# correct:  11366/20000 = 56.83%
# off by 1: 5657/20000 = 28.285%
epoch 38,	batch    10,	training loss: 1.205
epoch 38,	batch    20,	training loss: 1.288
epoch 38,	batch    30,	training loss: 1.439
epoch 38,	batch    40,	training loss: 1.002
epoch 38,	batch    50,	training loss: 0.954
epoch 38,	batch    60,	training loss: 1.655
epoch 38,	batch    70,	training loss: 1.372
epoch 38,	batch    80,	training loss: 3.441
epoch 38,	batch    90,	training loss: 2.941
epoch 38,	batch   100,	training loss: 2.371
epoch 38,	batch   110,	training loss: 2.689
epoch 38,	batch   120,	training loss: 3.497
epoch 38,	batch   130,	training loss: 2.642
epoch 38,	batch   140,	training loss: 1.742
epoch 38,	batch   150,	training loss: 1.579
epoch 38,	batch   160,	training loss: 2.135
epoch 38,	batch   170,	training loss: 1.138
epoch 38,	batch   180,	training loss: 0.962
epoch 38,	batch   190,	training loss: 1.096
epoch 38,	batch   200,	training loss: 1.075
epoch 38,	batch   210,	training loss: 1.185
epoch 38,	batch   220,	training loss: 0.841
epoch 38,	batch   230,	training loss: 1.758
epoch 38,	batch   240,	training loss: 1.561
epoch 38,	batch   250,	training loss: 1.960
epoch 38,	batch   260,	training loss: 1.878
epoch 38,	batch   270,	training loss: 1.221
epoch 38,	batch   280,	training loss: 1.332
epoch 38,	batch   290,	training loss: 1.311
epoch 38,	batch   300,	training loss: 0.826
epoch 38,	batch   310,	training loss: 0.878
end of epoch 38
testing on validation set:
# correct:  11249/20000 = 56.245%
# off by 1: 5631/20000 = 28.155%
epoch 39,	batch    10,	training loss: 1.617
epoch 39,	batch    20,	training loss: 1.632
epoch 39,	batch    30,	training loss: 0.916
epoch 39,	batch    40,	training loss: 1.164
epoch 39,	batch    50,	training loss: 1.009
epoch 39,	batch    60,	training loss: 0.980
epoch 39,	batch    70,	training loss: 1.110
epoch 39,	batch    80,	training loss: 1.546
epoch 39,	batch    90,	training loss: 0.990
epoch 39,	batch   100,	training loss: 0.936
epoch 39,	batch   110,	training loss: 0.988
epoch 39,	batch   120,	training loss: 0.885
epoch 39,	batch   130,	training loss: 0.929
epoch 39,	batch   140,	training loss: 0.812
epoch 39,	batch   150,	training loss: 0.809
epoch 39,	batch   160,	training loss: 0.972
epoch 39,	batch   170,	training loss: 1.033
epoch 39,	batch   180,	training loss: 0.769
epoch 39,	batch   190,	training loss: 0.801
epoch 39,	batch   200,	training loss: 0.719
epoch 39,	batch   210,	training loss: 0.701
epoch 39,	batch   220,	training loss: 1.112
epoch 39,	batch   230,	training loss: 1.233
epoch 39,	batch   240,	training loss: 1.141
epoch 39,	batch   250,	training loss: 1.234
epoch 39,	batch   260,	training loss: 1.330
epoch 39,	batch   270,	training loss: 1.250
epoch 39,	batch   280,	training loss: 1.010
epoch 39,	batch   290,	training loss: 0.746
epoch 39,	batch   300,	training loss: 1.138
epoch 39,	batch   310,	training loss: 1.076
end of epoch 39
testing on validation set:
# correct:  12354/20000 = 61.77%
# off by 1: 5194/20000 = 25.97%
epoch 40,	batch    10,	training loss: 1.421
epoch 40,	batch    20,	training loss: 1.226
epoch 40,	batch    30,	training loss: 0.887
epoch 40,	batch    40,	training loss: 0.937
epoch 40,	batch    50,	training loss: 0.997
epoch 40,	batch    60,	training loss: 1.201
epoch 40,	batch    70,	training loss: 1.481
epoch 40,	batch    80,	training loss: 1.062
epoch 40,	batch    90,	training loss: 0.841
epoch 40,	batch   100,	training loss: 1.246
epoch 40,	batch   110,	training loss: 1.174
epoch 40,	batch   120,	training loss: 1.281
epoch 40,	batch   130,	training loss: 0.962
epoch 40,	batch   140,	training loss: 0.657
epoch 40,	batch   150,	training loss: 0.647
epoch 40,	batch   160,	training loss: 0.729
epoch 40,	batch   170,	training loss: 0.727
epoch 40,	batch   180,	training loss: 0.843
epoch 40,	batch   190,	training loss: 0.970
epoch 40,	batch   200,	training loss: 1.000
epoch 40,	batch   210,	training loss: 1.331
epoch 40,	batch   220,	training loss: 0.984
epoch 40,	batch   230,	training loss: 0.946
epoch 40,	batch   240,	training loss: 0.893
epoch 40,	batch   250,	training loss: 0.749
epoch 40,	batch   260,	training loss: 0.668
epoch 40,	batch   270,	training loss: 0.735
epoch 40,	batch   280,	training loss: 0.693
epoch 40,	batch   290,	training loss: 0.558
epoch 40,	batch   300,	training loss: 0.761
epoch 40,	batch   310,	training loss: 0.598
end of epoch 40
testing on validation set:
# correct:  12264/20000 = 61.32%
# off by 1: 5197/20000 = 25.985%
epoch 41,	batch    10,	training loss: 0.733
epoch 41,	batch    20,	training loss: 1.292
epoch 41,	batch    30,	training loss: 1.402
epoch 41,	batch    40,	training loss: 1.350
epoch 41,	batch    50,	training loss: 1.070
epoch 41,	batch    60,	training loss: 0.833
epoch 41,	batch    70,	training loss: 1.013
epoch 41,	batch    80,	training loss: 0.677
epoch 41,	batch    90,	training loss: 0.713
epoch 41,	batch   100,	training loss: 1.086
epoch 41,	batch   110,	training loss: 1.184
epoch 41,	batch   120,	training loss: 1.230
epoch 41,	batch   130,	training loss: 0.701
epoch 41,	batch   140,	training loss: 0.809
epoch 41,	batch   150,	training loss: 0.950
epoch 41,	batch   160,	training loss: 1.406
epoch 41,	batch   170,	training loss: 0.902
epoch 41,	batch   180,	training loss: 0.775
epoch 41,	batch   190,	training loss: 0.731
epoch 41,	batch   200,	training loss: 1.709
epoch 41,	batch   210,	training loss: 1.376
epoch 41,	batch   220,	training loss: 0.916
epoch 41,	batch   230,	training loss: 1.074
epoch 41,	batch   240,	training loss: 0.931
epoch 41,	batch   250,	training loss: 0.881
epoch 41,	batch   260,	training loss: 0.742
epoch 41,	batch   270,	training loss: 0.814
epoch 41,	batch   280,	training loss: 0.922
epoch 41,	batch   290,	training loss: 0.839
epoch 41,	batch   300,	training loss: 1.164
epoch 41,	batch   310,	training loss: 0.829
end of epoch 41
testing on validation set:
# correct:  13381/20000 = 66.905%
# off by 1: 4628/20000 = 23.14%
epoch 42,	batch    10,	training loss: 1.111
epoch 42,	batch    20,	training loss: 1.151
epoch 42,	batch    30,	training loss: 0.784
epoch 42,	batch    40,	training loss: 1.045
epoch 42,	batch    50,	training loss: 0.779
epoch 42,	batch    60,	training loss: 0.881
epoch 42,	batch    70,	training loss: 1.091
epoch 42,	batch    80,	training loss: 1.165
epoch 42,	batch    90,	training loss: 1.299
epoch 42,	batch   100,	training loss: 0.838
epoch 42,	batch   110,	training loss: 0.842
epoch 42,	batch   120,	training loss: 0.730
epoch 42,	batch   130,	training loss: 0.576
epoch 42,	batch   140,	training loss: 0.930
epoch 42,	batch   150,	training loss: 1.059
epoch 42,	batch   160,	training loss: 1.878
epoch 42,	batch   170,	training loss: 1.488
epoch 42,	batch   180,	training loss: 0.967
epoch 42,	batch   190,	training loss: 0.824
epoch 42,	batch   200,	training loss: 0.650
epoch 42,	batch   210,	training loss: 0.908
epoch 42,	batch   220,	training loss: 0.843
epoch 42,	batch   230,	training loss: 0.955
epoch 42,	batch   240,	training loss: 0.995
epoch 42,	batch   250,	training loss: 0.854
epoch 42,	batch   260,	training loss: 0.960
epoch 42,	batch   270,	training loss: 0.990
epoch 42,	batch   280,	training loss: 0.885
epoch 42,	batch   290,	training loss: 0.806
epoch 42,	batch   300,	training loss: 0.652
epoch 42,	batch   310,	training loss: 0.823
end of epoch 42
testing on validation set:
# correct:  12556/20000 = 62.78%
# off by 1: 5032/20000 = 25.16%
epoch 43,	batch    10,	training loss: 0.906
epoch 43,	batch    20,	training loss: 1.061
epoch 43,	batch    30,	training loss: 1.441
epoch 43,	batch    40,	training loss: 1.221
epoch 43,	batch    50,	training loss: 1.035
epoch 43,	batch    60,	training loss: 0.916
epoch 43,	batch    70,	training loss: 0.836
epoch 43,	batch    80,	training loss: 0.795
epoch 43,	batch    90,	training loss: 0.738
epoch 43,	batch   100,	training loss: 0.986
epoch 43,	batch   110,	training loss: 0.786
epoch 43,	batch   120,	training loss: 0.743
epoch 43,	batch   130,	training loss: 0.744
epoch 43,	batch   140,	training loss: 0.560
epoch 43,	batch   150,	training loss: 0.603
epoch 43,	batch   160,	training loss: 0.623
epoch 43,	batch   170,	training loss: 0.585
epoch 43,	batch   180,	training loss: 0.659
epoch 43,	batch   190,	training loss: 0.754
epoch 43,	batch   200,	training loss: 0.789
epoch 43,	batch   210,	training loss: 0.683
epoch 43,	batch   220,	training loss: 0.790
epoch 43,	batch   230,	training loss: 0.836
epoch 43,	batch   240,	training loss: 1.392
epoch 43,	batch   250,	training loss: 1.094
epoch 43,	batch   260,	training loss: 0.886
epoch 43,	batch   270,	training loss: 0.748
epoch 43,	batch   280,	training loss: 0.760
epoch 43,	batch   290,	training loss: 0.668
epoch 43,	batch   300,	training loss: 0.876
epoch 43,	batch   310,	training loss: 1.060
end of epoch 43
testing on validation set:
# correct:  11260/20000 = 56.3%
# off by 1: 5663/20000 = 28.315%
epoch 44,	batch    10,	training loss: 0.782
epoch 44,	batch    20,	training loss: 0.591
epoch 44,	batch    30,	training loss: 0.670
epoch 44,	batch    40,	training loss: 0.694
epoch 44,	batch    50,	training loss: 1.287
epoch 44,	batch    60,	training loss: 0.763
epoch 44,	batch    70,	training loss: 0.928
epoch 44,	batch    80,	training loss: 0.513
epoch 44,	batch    90,	training loss: 0.761
epoch 44,	batch   100,	training loss: 0.743
epoch 44,	batch   110,	training loss: 0.586
epoch 44,	batch   120,	training loss: 0.876
epoch 44,	batch   130,	training loss: 0.719
epoch 44,	batch   140,	training loss: 0.691
epoch 44,	batch   150,	training loss: 0.568
epoch 44,	batch   160,	training loss: 1.175
epoch 44,	batch   170,	training loss: 0.744
epoch 44,	batch   180,	training loss: 1.028
epoch 44,	batch   190,	training loss: 0.872
epoch 44,	batch   200,	training loss: 0.814
epoch 44,	batch   210,	training loss: 0.941
epoch 44,	batch   220,	training loss: 0.990
epoch 44,	batch   230,	training loss: 0.694
epoch 44,	batch   240,	training loss: 0.674
epoch 44,	batch   250,	training loss: 0.549
epoch 44,	batch   260,	training loss: 0.684
epoch 44,	batch   270,	training loss: 0.716
epoch 44,	batch   280,	training loss: 0.740
epoch 44,	batch   290,	training loss: 0.784
epoch 44,	batch   300,	training loss: 1.162
epoch 44,	batch   310,	training loss: 1.070
end of epoch 44
testing on validation set:
# correct:  12403/20000 = 62.015%
# off by 1: 5005/20000 = 25.025%
epoch 45,	batch    10,	training loss: 0.807
epoch 45,	batch    20,	training loss: 0.707
epoch 45,	batch    30,	training loss: 0.773
epoch 45,	batch    40,	training loss: 0.870
epoch 45,	batch    50,	training loss: 1.013
epoch 45,	batch    60,	training loss: 0.740
epoch 45,	batch    70,	training loss: 0.816
epoch 45,	batch    80,	training loss: 1.514
epoch 45,	batch    90,	training loss: 1.029
epoch 45,	batch   100,	training loss: 0.998
epoch 45,	batch   110,	training loss: 1.007
epoch 45,	batch   120,	training loss: 0.841
epoch 45,	batch   130,	training loss: 0.802
epoch 45,	batch   140,	training loss: 0.653
epoch 45,	batch   150,	training loss: 0.548
epoch 45,	batch   160,	training loss: 0.532
epoch 45,	batch   170,	training loss: 0.814
epoch 45,	batch   180,	training loss: 2.006
epoch 45,	batch   190,	training loss: 1.230
epoch 45,	batch   200,	training loss: 0.968
epoch 45,	batch   210,	training loss: 0.784
epoch 45,	batch   220,	training loss: 0.871
epoch 45,	batch   230,	training loss: 0.829
epoch 45,	batch   240,	training loss: 0.887
epoch 45,	batch   250,	training loss: 1.026
epoch 45,	batch   260,	training loss: 0.955
epoch 45,	batch   270,	training loss: 0.871
epoch 45,	batch   280,	training loss: 0.825
epoch 45,	batch   290,	training loss: 0.651
epoch 45,	batch   300,	training loss: 0.906
epoch 45,	batch   310,	training loss: 1.414
end of epoch 45
testing on validation set:
# correct:  12046/20000 = 60.23%
# off by 1: 5132/20000 = 25.66%
epoch 46,	batch    10,	training loss: 0.698
epoch 46,	batch    20,	training loss: 0.531
epoch 46,	batch    30,	training loss: 0.616
epoch 46,	batch    40,	training loss: 0.781
epoch 46,	batch    50,	training loss: 0.663
epoch 46,	batch    60,	training loss: 0.771
epoch 46,	batch    70,	training loss: 0.622
epoch 46,	batch    80,	training loss: 0.618
epoch 46,	batch    90,	training loss: 0.642
epoch 46,	batch   100,	training loss: 0.915
epoch 46,	batch   110,	training loss: 1.256
epoch 46,	batch   120,	training loss: 1.287
epoch 46,	batch   130,	training loss: 0.679
epoch 46,	batch   140,	training loss: 0.802
epoch 46,	batch   150,	training loss: 0.642
epoch 46,	batch   160,	training loss: 0.621
epoch 46,	batch   170,	training loss: 0.604
epoch 46,	batch   180,	training loss: 0.585
epoch 46,	batch   190,	training loss: 0.875
epoch 46,	batch   200,	training loss: 0.589
epoch 46,	batch   210,	training loss: 0.718
epoch 46,	batch   220,	training loss: 0.824
epoch 46,	batch   230,	training loss: 0.715
epoch 46,	batch   240,	training loss: 0.893
epoch 46,	batch   250,	training loss: 1.109
epoch 46,	batch   260,	training loss: 0.770
epoch 46,	batch   270,	training loss: 0.910
epoch 46,	batch   280,	training loss: 0.625
epoch 46,	batch   290,	training loss: 0.568
epoch 46,	batch   300,	training loss: 0.731
epoch 46,	batch   310,	training loss: 0.557
end of epoch 46
testing on validation set:
# correct:  13368/20000 = 66.84%
# off by 1: 4461/20000 = 22.305%
epoch 47,	batch    10,	training loss: 0.626
epoch 47,	batch    20,	training loss: 0.923
epoch 47,	batch    30,	training loss: 0.709
epoch 47,	batch    40,	training loss: 0.572
epoch 47,	batch    50,	training loss: 1.034
epoch 47,	batch    60,	training loss: 1.498
epoch 47,	batch    70,	training loss: 1.743
epoch 47,	batch    80,	training loss: 0.966
epoch 47,	batch    90,	training loss: 0.895
epoch 47,	batch   100,	training loss: 0.874
epoch 47,	batch   110,	training loss: 1.078
epoch 47,	batch   120,	training loss: 0.762
epoch 47,	batch   130,	training loss: 0.971
epoch 47,	batch   140,	training loss: 1.207
epoch 47,	batch   150,	training loss: 0.812
epoch 47,	batch   160,	training loss: 0.665
epoch 47,	batch   170,	training loss: 0.728
epoch 47,	batch   180,	training loss: 1.467
epoch 47,	batch   190,	training loss: 0.746
epoch 47,	batch   200,	training loss: 0.693
epoch 47,	batch   210,	training loss: 0.530
epoch 47,	batch   220,	training loss: 0.679
epoch 47,	batch   230,	training loss: 1.415
epoch 47,	batch   240,	training loss: 0.850
epoch 47,	batch   250,	training loss: 0.526
epoch 47,	batch   260,	training loss: 0.795
epoch 47,	batch   270,	training loss: 0.835
epoch 47,	batch   280,	training loss: 1.010
epoch 47,	batch   290,	training loss: 1.099
epoch 47,	batch   300,	training loss: 0.597
epoch 47,	batch   310,	training loss: 0.773
end of epoch 47
testing on validation set:
# correct:  12160/20000 = 60.8%
# off by 1: 5169/20000 = 25.845%
epoch 48,	batch    10,	training loss: 1.135
epoch 48,	batch    20,	training loss: 0.872
epoch 48,	batch    30,	training loss: 0.940
epoch 48,	batch    40,	training loss: 0.978
epoch 48,	batch    50,	training loss: 0.938
epoch 48,	batch    60,	training loss: 0.918
epoch 48,	batch    70,	training loss: 0.851
epoch 48,	batch    80,	training loss: 0.716
epoch 48,	batch    90,	training loss: 0.888
epoch 48,	batch   100,	training loss: 0.628
epoch 48,	batch   110,	training loss: 0.653
epoch 48,	batch   120,	training loss: 0.691
epoch 48,	batch   130,	training loss: 0.812
epoch 48,	batch   140,	training loss: 0.732
epoch 48,	batch   150,	training loss: 0.647
epoch 48,	batch   160,	training loss: 0.630
epoch 48,	batch   170,	training loss: 0.777
epoch 48,	batch   180,	training loss: 0.805
epoch 48,	batch   190,	training loss: 0.943
epoch 48,	batch   200,	training loss: 0.812
epoch 48,	batch   210,	training loss: 0.921
epoch 48,	batch   220,	training loss: 0.758
epoch 48,	batch   230,	training loss: 1.141
epoch 48,	batch   240,	training loss: 1.004
epoch 48,	batch   250,	training loss: 0.689
epoch 48,	batch   260,	training loss: 0.535
epoch 48,	batch   270,	training loss: 0.588
epoch 48,	batch   280,	training loss: 0.622
epoch 48,	batch   290,	training loss: 0.698
epoch 48,	batch   300,	training loss: 0.573
epoch 48,	batch   310,	training loss: 1.069
end of epoch 48
testing on validation set:
# correct:  12172/20000 = 60.86%
# off by 1: 5295/20000 = 26.475%
epoch 49,	batch    10,	training loss: 1.166
epoch 49,	batch    20,	training loss: 0.928
epoch 49,	batch    30,	training loss: 0.679
epoch 49,	batch    40,	training loss: 0.635
epoch 49,	batch    50,	training loss: 1.066
epoch 49,	batch    60,	training loss: 1.188
epoch 49,	batch    70,	training loss: 1.653
epoch 49,	batch    80,	training loss: 0.939
epoch 49,	batch    90,	training loss: 0.727
epoch 49,	batch   100,	training loss: 0.954
epoch 49,	batch   110,	training loss: 0.762
epoch 49,	batch   120,	training loss: 0.676
epoch 49,	batch   130,	training loss: 0.869
epoch 49,	batch   140,	training loss: 1.269
epoch 49,	batch   150,	training loss: 0.966
epoch 49,	batch   160,	training loss: 0.740
epoch 49,	batch   170,	training loss: 0.744
epoch 49,	batch   180,	training loss: 1.957
epoch 49,	batch   190,	training loss: 1.730
epoch 49,	batch   200,	training loss: 2.310
epoch 49,	batch   210,	training loss: 1.936
epoch 49,	batch   220,	training loss: 1.386
epoch 49,	batch   230,	training loss: 1.102
epoch 49,	batch   240,	training loss: 0.691
epoch 49,	batch   250,	training loss: 0.673
epoch 49,	batch   260,	training loss: 0.885
epoch 49,	batch   270,	training loss: 0.767
epoch 49,	batch   280,	training loss: 1.914
epoch 49,	batch   290,	training loss: 1.983
epoch 49,	batch   300,	training loss: 1.506
epoch 49,	batch   310,	training loss: 1.041
end of epoch 49
testing on validation set:
# correct:  13003/20000 = 65.015%
# off by 1: 4864/20000 = 24.32%
epoch 50,	batch    10,	training loss: 0.689
epoch 50,	batch    20,	training loss: 0.803
epoch 50,	batch    30,	training loss: 0.847
epoch 50,	batch    40,	training loss: 0.612
epoch 50,	batch    50,	training loss: 0.667
epoch 50,	batch    60,	training loss: 0.611
epoch 50,	batch    70,	training loss: 0.745
epoch 50,	batch    80,	training loss: 1.175
epoch 50,	batch    90,	training loss: 0.559
epoch 50,	batch   100,	training loss: 0.588
epoch 50,	batch   110,	training loss: 0.793
epoch 50,	batch   120,	training loss: 0.671
epoch 50,	batch   130,	training loss: 0.519
epoch 50,	batch   140,	training loss: 0.741
epoch 50,	batch   150,	training loss: 0.613
epoch 50,	batch   160,	training loss: 0.755
epoch 50,	batch   170,	training loss: 0.995
epoch 50,	batch   180,	training loss: 0.766
epoch 50,	batch   190,	training loss: 0.742
epoch 50,	batch   200,	training loss: 0.913
epoch 50,	batch   210,	training loss: 1.543
epoch 50,	batch   220,	training loss: 1.609
epoch 50,	batch   230,	training loss: 1.207
epoch 50,	batch   240,	training loss: 0.956
epoch 50,	batch   250,	training loss: 1.090
epoch 50,	batch   260,	training loss: 1.063
epoch 50,	batch   270,	training loss: 1.474
epoch 50,	batch   280,	training loss: 0.939
epoch 50,	batch   290,	training loss: 0.852
epoch 50,	batch   300,	training loss: 0.983
epoch 50,	batch   310,	training loss: 1.122
end of epoch 50
testing on validation set:
# correct:  13172/20000 = 65.86%
# off by 1: 4675/20000 = 23.375%
epoch 51,	batch    10,	training loss: 1.101
epoch 51,	batch    20,	training loss: 0.757
epoch 51,	batch    30,	training loss: 0.528
epoch 51,	batch    40,	training loss: 0.654
epoch 51,	batch    50,	training loss: 0.661
epoch 51,	batch    60,	training loss: 0.495
epoch 51,	batch    70,	training loss: 0.620
epoch 51,	batch    80,	training loss: 0.503
epoch 51,	batch    90,	training loss: 0.738
epoch 51,	batch   100,	training loss: 0.732
epoch 51,	batch   110,	training loss: 1.154
epoch 51,	batch   120,	training loss: 1.227
epoch 51,	batch   130,	training loss: 0.726
epoch 51,	batch   140,	training loss: 0.557
epoch 51,	batch   150,	training loss: 0.735
epoch 51,	batch   160,	training loss: 0.665
epoch 51,	batch   170,	training loss: 0.632
epoch 51,	batch   180,	training loss: 1.067
epoch 51,	batch   190,	training loss: 0.764
epoch 51,	batch   200,	training loss: 0.654
epoch 51,	batch   210,	training loss: 0.657
epoch 51,	batch   220,	training loss: 0.578
epoch 51,	batch   230,	training loss: 0.608
epoch 51,	batch   240,	training loss: 0.483
epoch 51,	batch   250,	training loss: 0.705
epoch 51,	batch   260,	training loss: 0.791
epoch 51,	batch   270,	training loss: 0.765
epoch 51,	batch   280,	training loss: 0.727
epoch 51,	batch   290,	training loss: 0.635
epoch 51,	batch   300,	training loss: 0.898
epoch 51,	batch   310,	training loss: 0.734
end of epoch 51
testing on validation set:
# correct:  13041/20000 = 65.205%
# off by 1: 4791/20000 = 23.955%
epoch 52,	batch    10,	training loss: 0.659
epoch 52,	batch    20,	training loss: 0.790
epoch 52,	batch    30,	training loss: 0.621
epoch 52,	batch    40,	training loss: 0.683
epoch 52,	batch    50,	training loss: 0.697
epoch 52,	batch    60,	training loss: 0.581
epoch 52,	batch    70,	training loss: 0.607
epoch 52,	batch    80,	training loss: 0.805
epoch 52,	batch    90,	training loss: 0.903
epoch 52,	batch   100,	training loss: 0.451
epoch 52,	batch   110,	training loss: 0.551
epoch 52,	batch   120,	training loss: 0.620
epoch 52,	batch   130,	training loss: 0.707
epoch 52,	batch   140,	training loss: 0.757
epoch 52,	batch   150,	training loss: 0.679
epoch 52,	batch   160,	training loss: 1.005
epoch 52,	batch   170,	training loss: 1.321
epoch 52,	batch   180,	training loss: 1.084
epoch 52,	batch   190,	training loss: 0.770
epoch 52,	batch   200,	training loss: 0.541
epoch 52,	batch   210,	training loss: 0.767
epoch 52,	batch   220,	training loss: 0.549
epoch 52,	batch   230,	training loss: 0.503
epoch 52,	batch   240,	training loss: 0.695
epoch 52,	batch   250,	training loss: 0.797
epoch 52,	batch   260,	training loss: 0.813
epoch 52,	batch   270,	training loss: 0.718
epoch 52,	batch   280,	training loss: 0.589
epoch 52,	batch   290,	training loss: 0.685
epoch 52,	batch   300,	training loss: 0.669
epoch 52,	batch   310,	training loss: 0.598
end of epoch 52
testing on validation set:
# correct:  11881/20000 = 59.405%
# off by 1: 5265/20000 = 26.325%
epoch 53,	batch    10,	training loss: 0.734
epoch 53,	batch    20,	training loss: 0.638
epoch 53,	batch    30,	training loss: 0.994
epoch 53,	batch    40,	training loss: 1.371
epoch 53,	batch    50,	training loss: 1.094
epoch 53,	batch    60,	training loss: 1.207
epoch 53,	batch    70,	training loss: 0.800
epoch 53,	batch    80,	training loss: 0.738
epoch 53,	batch    90,	training loss: 0.638
epoch 53,	batch   100,	training loss: 0.697
epoch 53,	batch   110,	training loss: 0.500
epoch 53,	batch   120,	training loss: 0.561
epoch 53,	batch   130,	training loss: 0.588
epoch 53,	batch   140,	training loss: 0.527
epoch 53,	batch   150,	training loss: 0.674
epoch 53,	batch   160,	training loss: 0.928
epoch 53,	batch   170,	training loss: 1.151
epoch 53,	batch   180,	training loss: 0.864
epoch 53,	batch   190,	training loss: 0.913
epoch 53,	batch   200,	training loss: 0.739
epoch 53,	batch   210,	training loss: 0.612
epoch 53,	batch   220,	training loss: 0.508
epoch 53,	batch   230,	training loss: 0.783
epoch 53,	batch   240,	training loss: 0.902
epoch 53,	batch   250,	training loss: 1.168
epoch 53,	batch   260,	training loss: 1.047
epoch 53,	batch   270,	training loss: 0.803
epoch 53,	batch   280,	training loss: 0.901
epoch 53,	batch   290,	training loss: 0.758
epoch 53,	batch   300,	training loss: 0.700
epoch 53,	batch   310,	training loss: 0.849
end of epoch 53
testing on validation set:
# correct:  12622/20000 = 63.11%
# off by 1: 5124/20000 = 25.62%
epoch 54,	batch    10,	training loss: 0.969
epoch 54,	batch    20,	training loss: 0.702
epoch 54,	batch    30,	training loss: 0.582
epoch 54,	batch    40,	training loss: 0.796
epoch 54,	batch    50,	training loss: 0.866
epoch 54,	batch    60,	training loss: 0.798
epoch 54,	batch    70,	training loss: 0.626
epoch 54,	batch    80,	training loss: 0.684
epoch 54,	batch    90,	training loss: 0.787
epoch 54,	batch   100,	training loss: 0.731
epoch 54,	batch   110,	training loss: 0.676
epoch 54,	batch   120,	training loss: 0.569
epoch 54,	batch   130,	training loss: 0.433
epoch 54,	batch   140,	training loss: 0.587
epoch 54,	batch   150,	training loss: 0.554
epoch 54,	batch   160,	training loss: 0.836
epoch 54,	batch   170,	training loss: 0.588
epoch 54,	batch   180,	training loss: 0.638
epoch 54,	batch   190,	training loss: 1.347
epoch 54,	batch   200,	training loss: 0.732
epoch 54,	batch   210,	training loss: 0.606
epoch 54,	batch   220,	training loss: 0.765
epoch 54,	batch   230,	training loss: 0.710
epoch 54,	batch   240,	training loss: 0.498
epoch 54,	batch   250,	training loss: 0.686
epoch 54,	batch   260,	training loss: 0.529
epoch 54,	batch   270,	training loss: 0.467
epoch 54,	batch   280,	training loss: 0.699
epoch 54,	batch   290,	training loss: 1.031
epoch 54,	batch   300,	training loss: 1.292
epoch 54,	batch   310,	training loss: 1.290
end of epoch 54
testing on validation set:
# correct:  12042/20000 = 60.21%
# off by 1: 5295/20000 = 26.475%
epoch 55,	batch    10,	training loss: 1.057
epoch 55,	batch    20,	training loss: 0.849
epoch 55,	batch    30,	training loss: 1.017
epoch 55,	batch    40,	training loss: 0.682
epoch 55,	batch    50,	training loss: 0.590
epoch 55,	batch    60,	training loss: 0.896
epoch 55,	batch    70,	training loss: 1.066
epoch 55,	batch    80,	training loss: 0.750
epoch 55,	batch    90,	training loss: 0.923
epoch 55,	batch   100,	training loss: 0.919
epoch 55,	batch   110,	training loss: 0.746
epoch 55,	batch   120,	training loss: 0.927
epoch 55,	batch   130,	training loss: 0.750
epoch 55,	batch   140,	training loss: 0.613
epoch 55,	batch   150,	training loss: 0.561
epoch 55,	batch   160,	training loss: 0.463
epoch 55,	batch   170,	training loss: 0.526
epoch 55,	batch   180,	training loss: 0.596
epoch 55,	batch   190,	training loss: 0.584
epoch 55,	batch   200,	training loss: 0.711
epoch 55,	batch   210,	training loss: 0.548
epoch 55,	batch   220,	training loss: 0.531
epoch 55,	batch   230,	training loss: 0.497
epoch 55,	batch   240,	training loss: 0.551
epoch 55,	batch   250,	training loss: 0.474
epoch 55,	batch   260,	training loss: 0.474
epoch 55,	batch   270,	training loss: 0.669
epoch 55,	batch   280,	training loss: 0.710
epoch 55,	batch   290,	training loss: 0.650
epoch 55,	batch   300,	training loss: 0.721
epoch 55,	batch   310,	training loss: 0.574
end of epoch 55
testing on validation set:
# correct:  13156/20000 = 65.78%
# off by 1: 4642/20000 = 23.21%
epoch 56,	batch    10,	training loss: 0.727
epoch 56,	batch    20,	training loss: 1.042
epoch 56,	batch    30,	training loss: 0.784
epoch 56,	batch    40,	training loss: 0.769
epoch 56,	batch    50,	training loss: 1.035
epoch 56,	batch    60,	training loss: 1.057
epoch 56,	batch    70,	training loss: 0.837
epoch 56,	batch    80,	training loss: 0.907
epoch 56,	batch    90,	training loss: 0.609
epoch 56,	batch   100,	training loss: 0.633
epoch 56,	batch   110,	training loss: 0.552
epoch 56,	batch   120,	training loss: 0.557
epoch 56,	batch   130,	training loss: 0.554
epoch 56,	batch   140,	training loss: 0.658
epoch 56,	batch   150,	training loss: 1.181
epoch 56,	batch   160,	training loss: 0.574
epoch 56,	batch   170,	training loss: 0.573
epoch 56,	batch   180,	training loss: 1.139
epoch 56,	batch   190,	training loss: 0.745
epoch 56,	batch   200,	training loss: 0.747
epoch 56,	batch   210,	training loss: 0.957
epoch 56,	batch   220,	training loss: 1.419
epoch 56,	batch   230,	training loss: 0.660
epoch 56,	batch   240,	training loss: 0.526
epoch 56,	batch   250,	training loss: 0.409
epoch 56,	batch   260,	training loss: 0.492
epoch 56,	batch   270,	training loss: 0.554
epoch 56,	batch   280,	training loss: 0.637
epoch 56,	batch   290,	training loss: 0.905
epoch 56,	batch   300,	training loss: 0.743
epoch 56,	batch   310,	training loss: 0.533
end of epoch 56
testing on validation set:
# correct:  13253/20000 = 66.265%
# off by 1: 4638/20000 = 23.19%
epoch 57,	batch    10,	training loss: 0.677
epoch 57,	batch    20,	training loss: 0.996
epoch 57,	batch    30,	training loss: 0.499
epoch 57,	batch    40,	training loss: 0.780
epoch 57,	batch    50,	training loss: 0.732
epoch 57,	batch    60,	training loss: 0.705
epoch 57,	batch    70,	training loss: 0.608
epoch 57,	batch    80,	training loss: 0.475
epoch 57,	batch    90,	training loss: 0.698
epoch 57,	batch   100,	training loss: 0.807
epoch 57,	batch   110,	training loss: 0.808
epoch 57,	batch   120,	training loss: 0.812
epoch 57,	batch   130,	training loss: 0.664
epoch 57,	batch   140,	training loss: 0.555
epoch 57,	batch   150,	training loss: 0.539
epoch 57,	batch   160,	training loss: 0.399
epoch 57,	batch   170,	training loss: 1.096
epoch 57,	batch   180,	training loss: 1.817
epoch 57,	batch   190,	training loss: 1.201
epoch 57,	batch   200,	training loss: 0.921
epoch 57,	batch   210,	training loss: 1.086
epoch 57,	batch   220,	training loss: 0.769
epoch 57,	batch   230,	training loss: 0.633
epoch 57,	batch   240,	training loss: 0.617
epoch 57,	batch   250,	training loss: 0.738
epoch 57,	batch   260,	training loss: 0.795
epoch 57,	batch   270,	training loss: 0.676
epoch 57,	batch   280,	training loss: 0.669
epoch 57,	batch   290,	training loss: 0.776
epoch 57,	batch   300,	training loss: 0.719
epoch 57,	batch   310,	training loss: 0.867
end of epoch 57
testing on validation set:
# correct:  12526/20000 = 62.63%
# off by 1: 5008/20000 = 25.04%
epoch 58,	batch    10,	training loss: 0.732
epoch 58,	batch    20,	training loss: 0.708
epoch 58,	batch    30,	training loss: 0.558
epoch 58,	batch    40,	training loss: 0.828
epoch 58,	batch    50,	training loss: 0.592
epoch 58,	batch    60,	training loss: 0.694
epoch 58,	batch    70,	training loss: 0.815
epoch 58,	batch    80,	training loss: 0.698
epoch 58,	batch    90,	training loss: 0.491
epoch 58,	batch   100,	training loss: 0.417
epoch 58,	batch   110,	training loss: 0.497
epoch 58,	batch   120,	training loss: 0.635
epoch 58,	batch   130,	training loss: 0.571
epoch 58,	batch   140,	training loss: 0.534
epoch 58,	batch   150,	training loss: 0.513
epoch 58,	batch   160,	training loss: 0.725
epoch 58,	batch   170,	training loss: 0.897
epoch 58,	batch   180,	training loss: 0.538
epoch 58,	batch   190,	training loss: 0.495
epoch 58,	batch   200,	training loss: 0.871
epoch 58,	batch   210,	training loss: 0.941
epoch 58,	batch   220,	training loss: 0.768
epoch 58,	batch   230,	training loss: 0.840
epoch 58,	batch   240,	training loss: 0.735
epoch 58,	batch   250,	training loss: 0.805
epoch 58,	batch   260,	training loss: 0.755
epoch 58,	batch   270,	training loss: 0.597
epoch 58,	batch   280,	training loss: 0.943
epoch 58,	batch   290,	training loss: 0.944
epoch 58,	batch   300,	training loss: 0.775
epoch 58,	batch   310,	training loss: 1.032
end of epoch 58
testing on validation set:
# correct:  12979/20000 = 64.895%
# off by 1: 4705/20000 = 23.525%
epoch 59,	batch    10,	training loss: 0.977
epoch 59,	batch    20,	training loss: 0.654
epoch 59,	batch    30,	training loss: 0.698
epoch 59,	batch    40,	training loss: 0.677
epoch 59,	batch    50,	training loss: 0.488
epoch 59,	batch    60,	training loss: 0.491
epoch 59,	batch    70,	training loss: 0.539
epoch 59,	batch    80,	training loss: 0.467
epoch 59,	batch    90,	training loss: 0.391
epoch 59,	batch   100,	training loss: 0.627
epoch 59,	batch   110,	training loss: 0.823
epoch 59,	batch   120,	training loss: 0.522
epoch 59,	batch   130,	training loss: 0.808
epoch 59,	batch   140,	training loss: 0.900
epoch 59,	batch   150,	training loss: 0.533
epoch 59,	batch   160,	training loss: 0.485
epoch 59,	batch   170,	training loss: 0.521
epoch 59,	batch   180,	training loss: 0.582
epoch 59,	batch   190,	training loss: 0.472
epoch 59,	batch   200,	training loss: 0.402
epoch 59,	batch   210,	training loss: 0.375
epoch 59,	batch   220,	training loss: 0.465
epoch 59,	batch   230,	training loss: 0.379
epoch 59,	batch   240,	training loss: 0.513
epoch 59,	batch   250,	training loss: 0.496
epoch 59,	batch   260,	training loss: 0.416
epoch 59,	batch   270,	training loss: 0.546
epoch 59,	batch   280,	training loss: 0.951
epoch 59,	batch   290,	training loss: 0.921
epoch 59,	batch   300,	training loss: 0.614
epoch 59,	batch   310,	training loss: 0.492
end of epoch 59
testing on validation set:
# correct:  12237/20000 = 61.185%
# off by 1: 5308/20000 = 26.54%
epoch 60,	batch    10,	training loss: 0.461
epoch 60,	batch    20,	training loss: 0.582
epoch 60,	batch    30,	training loss: 1.004
epoch 60,	batch    40,	training loss: 0.653
epoch 60,	batch    50,	training loss: 0.505
epoch 60,	batch    60,	training loss: 0.484
epoch 60,	batch    70,	training loss: 0.665
epoch 60,	batch    80,	training loss: 0.529
epoch 60,	batch    90,	training loss: 0.429
epoch 60,	batch   100,	training loss: 0.404
epoch 60,	batch   110,	training loss: 0.530
epoch 60,	batch   120,	training loss: 0.452
epoch 60,	batch   130,	training loss: 0.577
epoch 60,	batch   140,	training loss: 0.488
epoch 60,	batch   150,	training loss: 0.432
epoch 60,	batch   160,	training loss: 0.559
epoch 60,	batch   170,	training loss: 0.575
epoch 60,	batch   180,	training loss: 0.566
epoch 60,	batch   190,	training loss: 0.919
epoch 60,	batch   200,	training loss: 1.063
epoch 60,	batch   210,	training loss: 0.704
epoch 60,	batch   220,	training loss: 0.621
epoch 60,	batch   230,	training loss: 0.809
epoch 60,	batch   240,	training loss: 0.775
epoch 60,	batch   250,	training loss: 0.858
epoch 60,	batch   260,	training loss: 0.762
epoch 60,	batch   270,	training loss: 1.544
epoch 60,	batch   280,	training loss: 1.392
epoch 60,	batch   290,	training loss: 1.360
epoch 60,	batch   300,	training loss: 1.112
epoch 60,	batch   310,	training loss: 0.754
end of epoch 60
testing on validation set:
# correct:  12647/20000 = 63.235%
# off by 1: 5019/20000 = 25.095%
epoch 61,	batch    10,	training loss: 0.447
epoch 61,	batch    20,	training loss: 0.688
epoch 61,	batch    30,	training loss: 0.456
epoch 61,	batch    40,	training loss: 0.463
epoch 61,	batch    50,	training loss: 0.459
epoch 61,	batch    60,	training loss: 0.440
epoch 61,	batch    70,	training loss: 0.499
epoch 61,	batch    80,	training loss: 0.438
epoch 61,	batch    90,	training loss: 0.701
epoch 61,	batch   100,	training loss: 0.651
epoch 61,	batch   110,	training loss: 0.554
epoch 61,	batch   120,	training loss: 0.456
epoch 61,	batch   130,	training loss: 0.732
epoch 61,	batch   140,	training loss: 0.541
epoch 61,	batch   150,	training loss: 0.481
epoch 61,	batch   160,	training loss: 0.511
epoch 61,	batch   170,	training loss: 0.685
epoch 61,	batch   180,	training loss: 0.560
epoch 61,	batch   190,	training loss: 0.476
epoch 61,	batch   200,	training loss: 0.556
epoch 61,	batch   210,	training loss: 0.528
epoch 61,	batch   220,	training loss: 0.478
epoch 61,	batch   230,	training loss: 0.471
epoch 61,	batch   240,	training loss: 0.708
epoch 61,	batch   250,	training loss: 0.752
epoch 61,	batch   260,	training loss: 1.142
epoch 61,	batch   270,	training loss: 0.748
epoch 61,	batch   280,	training loss: 0.698
epoch 61,	batch   290,	training loss: 1.237
epoch 61,	batch   300,	training loss: 0.949
epoch 61,	batch   310,	training loss: 0.603
end of epoch 61
testing on validation set:
# correct:  13242/20000 = 66.21%
# off by 1: 4576/20000 = 22.88%
epoch 62,	batch    10,	training loss: 0.549
epoch 62,	batch    20,	training loss: 0.570
epoch 62,	batch    30,	training loss: 0.711
epoch 62,	batch    40,	training loss: 0.598
epoch 62,	batch    50,	training loss: 0.678
epoch 62,	batch    60,	training loss: 0.554
epoch 62,	batch    70,	training loss: 0.488
epoch 62,	batch    80,	training loss: 0.668
epoch 62,	batch    90,	training loss: 0.690
epoch 62,	batch   100,	training loss: 0.704
epoch 62,	batch   110,	training loss: 0.772
epoch 62,	batch   120,	training loss: 1.078
epoch 62,	batch   130,	training loss: 0.685
epoch 62,	batch   140,	training loss: 0.519
epoch 62,	batch   150,	training loss: 0.759
epoch 62,	batch   160,	training loss: 0.711
epoch 62,	batch   170,	training loss: 0.685
epoch 62,	batch   180,	training loss: 0.570
epoch 62,	batch   190,	training loss: 0.518
epoch 62,	batch   200,	training loss: 0.341
epoch 62,	batch   210,	training loss: 0.391
epoch 62,	batch   220,	training loss: 0.462
epoch 62,	batch   230,	training loss: 0.523
epoch 62,	batch   240,	training loss: 0.417
epoch 62,	batch   250,	training loss: 0.508
epoch 62,	batch   260,	training loss: 0.514
epoch 62,	batch   270,	training loss: 0.516
epoch 62,	batch   280,	training loss: 0.344
epoch 62,	batch   290,	training loss: 0.393
epoch 62,	batch   300,	training loss: 0.486
epoch 62,	batch   310,	training loss: 0.486
end of epoch 62
testing on validation set:
# correct:  13055/20000 = 65.275%
# off by 1: 4713/20000 = 23.565%
epoch 63,	batch    10,	training loss: 0.457
epoch 63,	batch    20,	training loss: 0.400
epoch 63,	batch    30,	training loss: 0.380
epoch 63,	batch    40,	training loss: 0.556
epoch 63,	batch    50,	training loss: 0.501
epoch 63,	batch    60,	training loss: 0.525
epoch 63,	batch    70,	training loss: 0.509
epoch 63,	batch    80,	training loss: 0.705
epoch 63,	batch    90,	training loss: 2.396
epoch 63,	batch   100,	training loss: 1.955
epoch 63,	batch   110,	training loss: 1.326
epoch 63,	batch   120,	training loss: 1.534
epoch 63,	batch   130,	training loss: 1.184
epoch 63,	batch   140,	training loss: 1.297
epoch 63,	batch   150,	training loss: 1.435
epoch 63,	batch   160,	training loss: 0.860
epoch 63,	batch   170,	training loss: 0.631
epoch 63,	batch   180,	training loss: 0.745
epoch 63,	batch   190,	training loss: 0.796
epoch 63,	batch   200,	training loss: 0.922
epoch 63,	batch   210,	training loss: 1.020
epoch 63,	batch   220,	training loss: 0.858
epoch 63,	batch   230,	training loss: 0.717
epoch 63,	batch   240,	training loss: 0.545
epoch 63,	batch   250,	training loss: 0.570
epoch 63,	batch   260,	training loss: 0.630
epoch 63,	batch   270,	training loss: 0.998
epoch 63,	batch   280,	training loss: 0.518
epoch 63,	batch   290,	training loss: 0.681
epoch 63,	batch   300,	training loss: 0.485
epoch 63,	batch   310,	training loss: 0.679
end of epoch 63
testing on validation set:
# correct:  13337/20000 = 66.685%
# off by 1: 4548/20000 = 22.74%
epoch 64,	batch    10,	training loss: 0.624
epoch 64,	batch    20,	training loss: 0.799
epoch 64,	batch    30,	training loss: 0.576
epoch 64,	batch    40,	training loss: 0.553
epoch 64,	batch    50,	training loss: 0.534
epoch 64,	batch    60,	training loss: 0.469
epoch 64,	batch    70,	training loss: 0.619
epoch 64,	batch    80,	training loss: 0.934
epoch 64,	batch    90,	training loss: 1.103
epoch 64,	batch   100,	training loss: 0.845
epoch 64,	batch   110,	training loss: 1.310
epoch 64,	batch   120,	training loss: 1.270
epoch 64,	batch   130,	training loss: 1.238
epoch 64,	batch   140,	training loss: 0.986
epoch 64,	batch   150,	training loss: 0.987
epoch 64,	batch   160,	training loss: 1.359
epoch 64,	batch   170,	training loss: 2.265
epoch 64,	batch   180,	training loss: 1.683
epoch 64,	batch   190,	training loss: 1.158
epoch 64,	batch   200,	training loss: 0.720
epoch 64,	batch   210,	training loss: 0.766
epoch 64,	batch   220,	training loss: 0.700
epoch 64,	batch   230,	training loss: 0.808
epoch 64,	batch   240,	training loss: 0.940
epoch 64,	batch   250,	training loss: 0.711
epoch 64,	batch   260,	training loss: 0.619
epoch 64,	batch   270,	training loss: 0.583
epoch 64,	batch   280,	training loss: 0.653
epoch 64,	batch   290,	training loss: 0.754
epoch 64,	batch   300,	training loss: 1.311
epoch 64,	batch   310,	training loss: 0.637
end of epoch 64
testing on validation set:
# correct:  13037/20000 = 65.185%
# off by 1: 4807/20000 = 24.035%
epoch 65,	batch    10,	training loss: 0.559
epoch 65,	batch    20,	training loss: 0.516
epoch 65,	batch    30,	training loss: 0.596
epoch 65,	batch    40,	training loss: 0.400
epoch 65,	batch    50,	training loss: 0.381
epoch 65,	batch    60,	training loss: 0.421
epoch 65,	batch    70,	training loss: 0.764
epoch 65,	batch    80,	training loss: 0.726
epoch 65,	batch    90,	training loss: 0.441
epoch 65,	batch   100,	training loss: 0.722
epoch 65,	batch   110,	training loss: 0.994
epoch 65,	batch   120,	training loss: 0.862
epoch 65,	batch   130,	training loss: 0.772
epoch 65,	batch   140,	training loss: 0.712
epoch 65,	batch   150,	training loss: 0.711
epoch 65,	batch   160,	training loss: 0.502
epoch 65,	batch   170,	training loss: 0.810
epoch 65,	batch   180,	training loss: 0.637
epoch 65,	batch   190,	training loss: 0.828
epoch 65,	batch   200,	training loss: 1.206
epoch 65,	batch   210,	training loss: 0.805
epoch 65,	batch   220,	training loss: 0.520
epoch 65,	batch   230,	training loss: 0.550
epoch 65,	batch   240,	training loss: 0.676
epoch 65,	batch   250,	training loss: 0.796
epoch 65,	batch   260,	training loss: 0.567
epoch 65,	batch   270,	training loss: 0.625
epoch 65,	batch   280,	training loss: 0.554
epoch 65,	batch   290,	training loss: 0.738
epoch 65,	batch   300,	training loss: 0.634
epoch 65,	batch   310,	training loss: 0.690
end of epoch 65
testing on validation set:
# correct:  13371/20000 = 66.855%
# off by 1: 4552/20000 = 22.76%
epoch 66,	batch    10,	training loss: 0.557
epoch 66,	batch    20,	training loss: 0.495
epoch 66,	batch    30,	training loss: 0.610
epoch 66,	batch    40,	training loss: 0.505
epoch 66,	batch    50,	training loss: 0.562
epoch 66,	batch    60,	training loss: 0.465
epoch 66,	batch    70,	training loss: 0.529
epoch 66,	batch    80,	training loss: 0.969
epoch 66,	batch    90,	training loss: 0.888
epoch 66,	batch   100,	training loss: 1.153
epoch 66,	batch   110,	training loss: 0.531
epoch 66,	batch   120,	training loss: 0.452
epoch 66,	batch   130,	training loss: 0.492
epoch 66,	batch   140,	training loss: 0.827
epoch 66,	batch   150,	training loss: 0.881
epoch 66,	batch   160,	training loss: 0.830
epoch 66,	batch   170,	training loss: 0.822
epoch 66,	batch   180,	training loss: 0.740
epoch 66,	batch   190,	training loss: 0.849
epoch 66,	batch   200,	training loss: 0.600
epoch 66,	batch   210,	training loss: 0.656
epoch 66,	batch   220,	training loss: 0.636
epoch 66,	batch   230,	training loss: 0.499
epoch 66,	batch   240,	training loss: 0.403
epoch 66,	batch   250,	training loss: 0.393
epoch 66,	batch   260,	training loss: 0.547
epoch 66,	batch   270,	training loss: 0.551
epoch 66,	batch   280,	training loss: 0.420
epoch 66,	batch   290,	training loss: 0.445
epoch 66,	batch   300,	training loss: 0.480
epoch 66,	batch   310,	training loss: 0.573
end of epoch 66
testing on validation set:
# correct:  13224/20000 = 66.12%
# off by 1: 4610/20000 = 23.05%
epoch 67,	batch    10,	training loss: 0.448
epoch 67,	batch    20,	training loss: 0.384
epoch 67,	batch    30,	training loss: 0.593
epoch 67,	batch    40,	training loss: 0.443
epoch 67,	batch    50,	training loss: 0.430
epoch 67,	batch    60,	training loss: 0.517
epoch 67,	batch    70,	training loss: 0.436
epoch 67,	batch    80,	training loss: 0.378
epoch 67,	batch    90,	training loss: 0.441
epoch 67,	batch   100,	training loss: 0.475
epoch 67,	batch   110,	training loss: 0.543
epoch 67,	batch   120,	training loss: 0.673
epoch 67,	batch   130,	training loss: 0.439
epoch 67,	batch   140,	training loss: 0.553
epoch 67,	batch   150,	training loss: 0.568
epoch 67,	batch   160,	training loss: 0.670
epoch 67,	batch   170,	training loss: 0.896
epoch 67,	batch   180,	training loss: 1.122
epoch 67,	batch   190,	training loss: 1.064
epoch 67,	batch   200,	training loss: 0.747
epoch 67,	batch   210,	training loss: 0.670
epoch 67,	batch   220,	training loss: 0.724
epoch 67,	batch   230,	training loss: 0.493
epoch 67,	batch   240,	training loss: 0.432
epoch 67,	batch   250,	training loss: 0.361
epoch 67,	batch   260,	training loss: 0.578
epoch 67,	batch   270,	training loss: 0.358
epoch 67,	batch   280,	training loss: 0.395
epoch 67,	batch   290,	training loss: 0.363
epoch 67,	batch   300,	training loss: 0.435
epoch 67,	batch   310,	training loss: 0.438
end of epoch 67
testing on validation set:
# correct:  12969/20000 = 64.845%
# off by 1: 4781/20000 = 23.905%
epoch 68,	batch    10,	training loss: 0.482
epoch 68,	batch    20,	training loss: 0.521
epoch 68,	batch    30,	training loss: 0.694
epoch 68,	batch    40,	training loss: 0.788
epoch 68,	batch    50,	training loss: 0.576
epoch 68,	batch    60,	training loss: 0.554
epoch 68,	batch    70,	training loss: 0.503
epoch 68,	batch    80,	training loss: 0.881
epoch 68,	batch    90,	training loss: 0.849
epoch 68,	batch   100,	training loss: 0.613
epoch 68,	batch   110,	training loss: 0.889
epoch 68,	batch   120,	training loss: 0.724
epoch 68,	batch   130,	training loss: 0.547
epoch 68,	batch   140,	training loss: 0.516
epoch 68,	batch   150,	training loss: 0.697
epoch 68,	batch   160,	training loss: 0.556
epoch 68,	batch   170,	training loss: 0.416
epoch 68,	batch   180,	training loss: 0.516
epoch 68,	batch   190,	training loss: 0.518
epoch 68,	batch   200,	training loss: 0.462
epoch 68,	batch   210,	training loss: 0.491
epoch 68,	batch   220,	training loss: 0.378
epoch 68,	batch   230,	training loss: 0.427
epoch 68,	batch   240,	training loss: 0.404
epoch 68,	batch   250,	training loss: 0.390
epoch 68,	batch   260,	training loss: 0.427
epoch 68,	batch   270,	training loss: 0.409
epoch 68,	batch   280,	training loss: 0.417
epoch 68,	batch   290,	training loss: 0.543
epoch 68,	batch   300,	training loss: 0.540
epoch 68,	batch   310,	training loss: 0.653
end of epoch 68
testing on validation set:
# correct:  13017/20000 = 65.085%
# off by 1: 4682/20000 = 23.41%
epoch 69,	batch    10,	training loss: 0.424
epoch 69,	batch    20,	training loss: 0.488
epoch 69,	batch    30,	training loss: 0.648
epoch 69,	batch    40,	training loss: 0.562
epoch 69,	batch    50,	training loss: 0.590
epoch 69,	batch    60,	training loss: 0.708
epoch 69,	batch    70,	training loss: 0.525
epoch 69,	batch    80,	training loss: 0.575
epoch 69,	batch    90,	training loss: 0.442
epoch 69,	batch   100,	training loss: 0.586
epoch 69,	batch   110,	training loss: 0.445
epoch 69,	batch   120,	training loss: 0.377
epoch 69,	batch   130,	training loss: 0.498
epoch 69,	batch   140,	training loss: 0.947
epoch 69,	batch   150,	training loss: 1.026
epoch 69,	batch   160,	training loss: 0.788
epoch 69,	batch   170,	training loss: 0.645
epoch 69,	batch   180,	training loss: 0.705
epoch 69,	batch   190,	training loss: 0.572
epoch 69,	batch   200,	training loss: 0.754
epoch 69,	batch   210,	training loss: 0.619
epoch 69,	batch   220,	training loss: 0.632
epoch 69,	batch   230,	training loss: 1.229
epoch 69,	batch   240,	training loss: 0.723
epoch 69,	batch   250,	training loss: 0.530
epoch 69,	batch   260,	training loss: 0.423
epoch 69,	batch   270,	training loss: 0.439
epoch 69,	batch   280,	training loss: 0.500
epoch 69,	batch   290,	training loss: 0.365
epoch 69,	batch   300,	training loss: 0.430
epoch 69,	batch   310,	training loss: 0.401
end of epoch 69
testing on validation set:
# correct:  13948/20000 = 69.74%
# off by 1: 4218/20000 = 21.09%
epoch 70,	batch    10,	training loss: 0.355
epoch 70,	batch    20,	training loss: 0.368
epoch 70,	batch    30,	training loss: 0.616
epoch 70,	batch    40,	training loss: 1.132
epoch 70,	batch    50,	training loss: 0.608
epoch 70,	batch    60,	training loss: 0.640
epoch 70,	batch    70,	training loss: 0.730
epoch 70,	batch    80,	training loss: 0.643
epoch 70,	batch    90,	training loss: 0.484
epoch 70,	batch   100,	training loss: 0.607
epoch 70,	batch   110,	training loss: 0.576
epoch 70,	batch   120,	training loss: 0.530
epoch 70,	batch   130,	training loss: 0.443
epoch 70,	batch   140,	training loss: 0.474
epoch 70,	batch   150,	training loss: 0.526
epoch 70,	batch   160,	training loss: 0.572
epoch 70,	batch   170,	training loss: 0.553
epoch 70,	batch   180,	training loss: 0.500
epoch 70,	batch   190,	training loss: 0.382
epoch 70,	batch   200,	training loss: 0.488
epoch 70,	batch   210,	training loss: 0.734
epoch 70,	batch   220,	training loss: 1.098
epoch 70,	batch   230,	training loss: 0.666
epoch 70,	batch   240,	training loss: 0.398
epoch 70,	batch   250,	training loss: 0.534
epoch 70,	batch   260,	training loss: 0.416
epoch 70,	batch   270,	training loss: 0.401
epoch 70,	batch   280,	training loss: 0.396
epoch 70,	batch   290,	training loss: 0.394
epoch 70,	batch   300,	training loss: 0.395
epoch 70,	batch   310,	training loss: 0.521
end of epoch 70
testing on validation set:
# correct:  13353/20000 = 66.765%
# off by 1: 4690/20000 = 23.45%
epoch 71,	batch    10,	training loss: 0.614
epoch 71,	batch    20,	training loss: 0.638
epoch 71,	batch    30,	training loss: 0.535
epoch 71,	batch    40,	training loss: 0.354
epoch 71,	batch    50,	training loss: 0.594
epoch 71,	batch    60,	training loss: 0.744
epoch 71,	batch    70,	training loss: 0.453
epoch 71,	batch    80,	training loss: 0.465
epoch 71,	batch    90,	training loss: 0.465
epoch 71,	batch   100,	training loss: 0.533
epoch 71,	batch   110,	training loss: 0.528
epoch 71,	batch   120,	training loss: 0.634
epoch 71,	batch   130,	training loss: 0.740
epoch 71,	batch   140,	training loss: 0.461
epoch 71,	batch   150,	training loss: 0.466
epoch 71,	batch   160,	training loss: 0.922
epoch 71,	batch   170,	training loss: 0.866
epoch 71,	batch   180,	training loss: 0.481
epoch 71,	batch   190,	training loss: 0.489
epoch 71,	batch   200,	training loss: 0.555
epoch 71,	batch   210,	training loss: 0.860
epoch 71,	batch   220,	training loss: 1.208
epoch 71,	batch   230,	training loss: 0.766
epoch 71,	batch   240,	training loss: 1.083
epoch 71,	batch   250,	training loss: 0.703
epoch 71,	batch   260,	training loss: 0.751
epoch 71,	batch   270,	training loss: 0.531
epoch 71,	batch   280,	training loss: 0.425
epoch 71,	batch   290,	training loss: 0.575
epoch 71,	batch   300,	training loss: 0.464
epoch 71,	batch   310,	training loss: 0.581
end of epoch 71
testing on validation set:
# correct:  13027/20000 = 65.135%
# off by 1: 4765/20000 = 23.825%
epoch 72,	batch    10,	training loss: 0.545
epoch 72,	batch    20,	training loss: 0.528
epoch 72,	batch    30,	training loss: 0.548
epoch 72,	batch    40,	training loss: 0.773
epoch 72,	batch    50,	training loss: 1.058
epoch 72,	batch    60,	training loss: 0.977
epoch 72,	batch    70,	training loss: 0.673
epoch 72,	batch    80,	training loss: 0.489
epoch 72,	batch    90,	training loss: 0.531
epoch 72,	batch   100,	training loss: 0.735
epoch 72,	batch   110,	training loss: 0.654
epoch 72,	batch   120,	training loss: 0.696
epoch 72,	batch   130,	training loss: 0.548
epoch 72,	batch   140,	training loss: 0.338
epoch 72,	batch   150,	training loss: 0.405
epoch 72,	batch   160,	training loss: 0.664
epoch 72,	batch   170,	training loss: 0.820
epoch 72,	batch   180,	training loss: 0.837
epoch 72,	batch   190,	training loss: 0.766
epoch 72,	batch   200,	training loss: 0.708
epoch 72,	batch   210,	training loss: 1.109
epoch 72,	batch   220,	training loss: 1.509
epoch 72,	batch   230,	training loss: 1.290
epoch 72,	batch   240,	training loss: 1.120
epoch 72,	batch   250,	training loss: 0.643
epoch 72,	batch   260,	training loss: 0.472
epoch 72,	batch   270,	training loss: 0.541
epoch 72,	batch   280,	training loss: 0.428
epoch 72,	batch   290,	training loss: 0.433
epoch 72,	batch   300,	training loss: 0.436
epoch 72,	batch   310,	training loss: 0.592
end of epoch 72
testing on validation set:
# correct:  13256/20000 = 66.28%
# off by 1: 4505/20000 = 22.525%
epoch 73,	batch    10,	training loss: 0.564
epoch 73,	batch    20,	training loss: 0.564
epoch 73,	batch    30,	training loss: 0.612
epoch 73,	batch    40,	training loss: 0.616
epoch 73,	batch    50,	training loss: 1.122
epoch 73,	batch    60,	training loss: 1.286
epoch 73,	batch    70,	training loss: 0.549
epoch 73,	batch    80,	training loss: 0.642
epoch 73,	batch    90,	training loss: 0.982
epoch 73,	batch   100,	training loss: 0.499
epoch 73,	batch   110,	training loss: 0.501
epoch 73,	batch   120,	training loss: 0.840
epoch 73,	batch   130,	training loss: 0.577
epoch 73,	batch   140,	training loss: 0.430
epoch 73,	batch   150,	training loss: 0.472
epoch 73,	batch   160,	training loss: 0.756
epoch 73,	batch   170,	training loss: 0.800
epoch 73,	batch   180,	training loss: 0.599
epoch 73,	batch   190,	training loss: 0.586
epoch 73,	batch   200,	training loss: 0.443
epoch 73,	batch   210,	training loss: 0.644
epoch 73,	batch   220,	training loss: 0.569
epoch 73,	batch   230,	training loss: 0.473
epoch 73,	batch   240,	training loss: 0.561
epoch 73,	batch   250,	training loss: 0.530
epoch 73,	batch   260,	training loss: 0.659
epoch 73,	batch   270,	training loss: 0.838
epoch 73,	batch   280,	training loss: 1.198
epoch 73,	batch   290,	training loss: 0.704
epoch 73,	batch   300,	training loss: 0.770
epoch 73,	batch   310,	training loss: 0.754
end of epoch 73
testing on validation set:
# correct:  13500/20000 = 67.5%
# off by 1: 4466/20000 = 22.33%
epoch 74,	batch    10,	training loss: 0.411
epoch 74,	batch    20,	training loss: 0.421
epoch 74,	batch    30,	training loss: 0.563
epoch 74,	batch    40,	training loss: 0.906
epoch 74,	batch    50,	training loss: 1.156
epoch 74,	batch    60,	training loss: 1.745
epoch 74,	batch    70,	training loss: 1.742
epoch 74,	batch    80,	training loss: 2.648
epoch 74,	batch    90,	training loss: 2.824
epoch 74,	batch   100,	training loss: 2.223
epoch 74,	batch   110,	training loss: 1.070
epoch 74,	batch   120,	training loss: 0.785
epoch 74,	batch   130,	training loss: 0.559
epoch 74,	batch   140,	training loss: 0.584
epoch 74,	batch   150,	training loss: 0.459
epoch 74,	batch   160,	training loss: 0.515
epoch 74,	batch   170,	training loss: 0.744
epoch 74,	batch   180,	training loss: 1.031
epoch 74,	batch   190,	training loss: 1.027
epoch 74,	batch   200,	training loss: 1.008
epoch 74,	batch   210,	training loss: 0.504
epoch 74,	batch   220,	training loss: 0.633
epoch 74,	batch   230,	training loss: 0.511
epoch 74,	batch   240,	training loss: 0.790
epoch 74,	batch   250,	training loss: 1.551
epoch 74,	batch   260,	training loss: 1.198
epoch 74,	batch   270,	training loss: 0.514
epoch 74,	batch   280,	training loss: 0.429
epoch 74,	batch   290,	training loss: 0.416
epoch 74,	batch   300,	training loss: 0.432
epoch 74,	batch   310,	training loss: 0.448
end of epoch 74
testing on validation set:
# correct:  11926/20000 = 59.63%
# off by 1: 5328/20000 = 26.64%
epoch 75,	batch    10,	training loss: 0.531
epoch 75,	batch    20,	training loss: 0.401
epoch 75,	batch    30,	training loss: 0.362
epoch 75,	batch    40,	training loss: 0.489
epoch 75,	batch    50,	training loss: 0.522
epoch 75,	batch    60,	training loss: 0.414
epoch 75,	batch    70,	training loss: 0.544
epoch 75,	batch    80,	training loss: 0.574
epoch 75,	batch    90,	training loss: 0.428
epoch 75,	batch   100,	training loss: 0.396
epoch 75,	batch   110,	training loss: 0.492
epoch 75,	batch   120,	training loss: 0.358
epoch 75,	batch   130,	training loss: 0.424
epoch 75,	batch   140,	training loss: 0.511
epoch 75,	batch   150,	training loss: 0.527
epoch 75,	batch   160,	training loss: 0.539
epoch 75,	batch   170,	training loss: 0.495
epoch 75,	batch   180,	training loss: 0.326
epoch 75,	batch   190,	training loss: 0.420
epoch 75,	batch   200,	training loss: 0.723
epoch 75,	batch   210,	training loss: 0.484
epoch 75,	batch   220,	training loss: 0.522
epoch 75,	batch   230,	training loss: 0.345
epoch 75,	batch   240,	training loss: 0.391
epoch 75,	batch   250,	training loss: 0.268
epoch 75,	batch   260,	training loss: 0.388
epoch 75,	batch   270,	training loss: 0.432
epoch 75,	batch   280,	training loss: 0.343
epoch 75,	batch   290,	training loss: 0.404
epoch 75,	batch   300,	training loss: 0.366
epoch 75,	batch   310,	training loss: 0.403
end of epoch 75
testing on validation set:
# correct:  13147/20000 = 65.735%
# off by 1: 4577/20000 = 22.885%
epoch 76,	batch    10,	training loss: 0.355
epoch 76,	batch    20,	training loss: 0.390
epoch 76,	batch    30,	training loss: 0.445
epoch 76,	batch    40,	training loss: 0.320
epoch 76,	batch    50,	training loss: 0.484
epoch 76,	batch    60,	training loss: 0.551
epoch 76,	batch    70,	training loss: 0.872
epoch 76,	batch    80,	training loss: 0.881
epoch 76,	batch    90,	training loss: 0.551
epoch 76,	batch   100,	training loss: 0.411
epoch 76,	batch   110,	training loss: 0.332
epoch 76,	batch   120,	training loss: 0.387
epoch 76,	batch   130,	training loss: 0.517
epoch 76,	batch   140,	training loss: 0.689
epoch 76,	batch   150,	training loss: 0.490
epoch 76,	batch   160,	training loss: 0.806
epoch 76,	batch   170,	training loss: 0.570
epoch 76,	batch   180,	training loss: 0.519
epoch 76,	batch   190,	training loss: 0.457
epoch 76,	batch   200,	training loss: 0.469
epoch 76,	batch   210,	training loss: 0.932
epoch 76,	batch   220,	training loss: 0.595
epoch 76,	batch   230,	training loss: 0.670
epoch 76,	batch   240,	training loss: 0.724
epoch 76,	batch   250,	training loss: 1.075
epoch 76,	batch   260,	training loss: 0.430
epoch 76,	batch   270,	training loss: 0.432
epoch 76,	batch   280,	training loss: 0.594
epoch 76,	batch   290,	training loss: 0.485
epoch 76,	batch   300,	training loss: 0.540
epoch 76,	batch   310,	training loss: 0.572
end of epoch 76
testing on validation set:
# correct:  10707/20000 = 53.535%
# off by 1: 6139/20000 = 30.695%
epoch 77,	batch    10,	training loss: 0.595
epoch 77,	batch    20,	training loss: 0.452
epoch 77,	batch    30,	training loss: 0.399
epoch 77,	batch    40,	training loss: 0.616
epoch 77,	batch    50,	training loss: 0.344
epoch 77,	batch    60,	training loss: 0.389
epoch 77,	batch    70,	training loss: 0.342
epoch 77,	batch    80,	training loss: 0.433
epoch 77,	batch    90,	training loss: 0.534
epoch 77,	batch   100,	training loss: 0.584
epoch 77,	batch   110,	training loss: 0.675
epoch 77,	batch   120,	training loss: 0.595
epoch 77,	batch   130,	training loss: 0.716
epoch 77,	batch   140,	training loss: 0.429
epoch 77,	batch   150,	training loss: 0.514
epoch 77,	batch   160,	training loss: 0.454
epoch 77,	batch   170,	training loss: 0.461
epoch 77,	batch   180,	training loss: 0.433
epoch 77,	batch   190,	training loss: 0.381
epoch 77,	batch   200,	training loss: 0.430
epoch 77,	batch   210,	training loss: 0.362
epoch 77,	batch   220,	training loss: 0.479
epoch 77,	batch   230,	training loss: 0.545
epoch 77,	batch   240,	training loss: 0.473
epoch 77,	batch   250,	training loss: 0.404
epoch 77,	batch   260,	training loss: 0.750
epoch 77,	batch   270,	training loss: 0.675
epoch 77,	batch   280,	training loss: 0.516
epoch 77,	batch   290,	training loss: 0.534
epoch 77,	batch   300,	training loss: 0.572
epoch 77,	batch   310,	training loss: 0.774
end of epoch 77
testing on validation set:
# correct:  13310/20000 = 66.55%
# off by 1: 4549/20000 = 22.745%
epoch 78,	batch    10,	training loss: 0.481
epoch 78,	batch    20,	training loss: 0.379
epoch 78,	batch    30,	training loss: 0.436
epoch 78,	batch    40,	training loss: 0.607
epoch 78,	batch    50,	training loss: 0.825
epoch 78,	batch    60,	training loss: 0.754
epoch 78,	batch    70,	training loss: 0.494
epoch 78,	batch    80,	training loss: 0.360
epoch 78,	batch    90,	training loss: 0.503
epoch 78,	batch   100,	training loss: 0.441
epoch 78,	batch   110,	training loss: 0.597
epoch 78,	batch   120,	training loss: 0.824
epoch 78,	batch   130,	training loss: 0.510
epoch 78,	batch   140,	training loss: 0.525
epoch 78,	batch   150,	training loss: 0.507
epoch 78,	batch   160,	training loss: 0.477
epoch 78,	batch   170,	training loss: 0.357
epoch 78,	batch   180,	training loss: 0.383
epoch 78,	batch   190,	training loss: 0.450
epoch 78,	batch   200,	training loss: 0.592
epoch 78,	batch   210,	training loss: 0.575
epoch 78,	batch   220,	training loss: 0.480
epoch 78,	batch   230,	training loss: 0.503
epoch 78,	batch   240,	training loss: 0.650
epoch 78,	batch   250,	training loss: 0.574
epoch 78,	batch   260,	training loss: 0.388
epoch 78,	batch   270,	training loss: 0.349
epoch 78,	batch   280,	training loss: 0.349
epoch 78,	batch   290,	training loss: 0.399
epoch 78,	batch   300,	training loss: 0.413
epoch 78,	batch   310,	training loss: 0.377
end of epoch 78
testing on validation set:
# correct:  13074/20000 = 65.37%
# off by 1: 4722/20000 = 23.61%
epoch 79,	batch    10,	training loss: 0.395
epoch 79,	batch    20,	training loss: 0.367
epoch 79,	batch    30,	training loss: 0.260
epoch 79,	batch    40,	training loss: 0.372
epoch 79,	batch    50,	training loss: 0.657
epoch 79,	batch    60,	training loss: 0.698
epoch 79,	batch    70,	training loss: 0.626
epoch 79,	batch    80,	training loss: 0.508
epoch 79,	batch    90,	training loss: 0.454
epoch 79,	batch   100,	training loss: 0.429
epoch 79,	batch   110,	training loss: 0.369
epoch 79,	batch   120,	training loss: 0.385
epoch 79,	batch   130,	training loss: 0.550
epoch 79,	batch   140,	training loss: 0.450
epoch 79,	batch   150,	training loss: 0.584
epoch 79,	batch   160,	training loss: 0.450
epoch 79,	batch   170,	training loss: 0.389
epoch 79,	batch   180,	training loss: 0.497
epoch 79,	batch   190,	training loss: 0.440
epoch 79,	batch   200,	training loss: 0.412
epoch 79,	batch   210,	training loss: 0.326
epoch 79,	batch   220,	training loss: 0.288
epoch 79,	batch   230,	training loss: 0.416
epoch 79,	batch   240,	training loss: 0.461
epoch 79,	batch   250,	training loss: 0.421
epoch 79,	batch   260,	training loss: 0.830
epoch 79,	batch   270,	training loss: 0.679
epoch 79,	batch   280,	training loss: 0.552
epoch 79,	batch   290,	training loss: 0.443
epoch 79,	batch   300,	training loss: 0.679
epoch 79,	batch   310,	training loss: 0.729
end of epoch 79
testing on validation set:
# correct:  13213/20000 = 66.065%
# off by 1: 4567/20000 = 22.835%
epoch 80,	batch    10,	training loss: 0.464
epoch 80,	batch    20,	training loss: 0.393
epoch 80,	batch    30,	training loss: 0.338
epoch 80,	batch    40,	training loss: 0.476
epoch 80,	batch    50,	training loss: 0.356
epoch 80,	batch    60,	training loss: 0.458
epoch 80,	batch    70,	training loss: 0.620
epoch 80,	batch    80,	training loss: 0.561
epoch 80,	batch    90,	training loss: 0.571
epoch 80,	batch   100,	training loss: 0.758
epoch 80,	batch   110,	training loss: 0.630
epoch 80,	batch   120,	training loss: 0.776
epoch 80,	batch   130,	training loss: 0.737
epoch 80,	batch   140,	training loss: 0.763
epoch 80,	batch   150,	training loss: 0.611
epoch 80,	batch   160,	training loss: 0.495
epoch 80,	batch   170,	training loss: 0.631
epoch 80,	batch   180,	training loss: 0.511
epoch 80,	batch   190,	training loss: 0.546
epoch 80,	batch   200,	training loss: 0.385
epoch 80,	batch   210,	training loss: 0.506
epoch 80,	batch   220,	training loss: 0.415
epoch 80,	batch   230,	training loss: 0.389
epoch 80,	batch   240,	training loss: 0.399
epoch 80,	batch   250,	training loss: 0.448
epoch 80,	batch   260,	training loss: 0.361
epoch 80,	batch   270,	training loss: 0.403
epoch 80,	batch   280,	training loss: 0.509
epoch 80,	batch   290,	training loss: 0.457
epoch 80,	batch   300,	training loss: 0.357
epoch 80,	batch   310,	training loss: 0.390
end of epoch 80
testing on validation set:
# correct:  13395/20000 = 66.975%
# off by 1: 4471/20000 = 22.355%
epoch 81,	batch    10,	training loss: 0.487
epoch 81,	batch    20,	training loss: 0.626
epoch 81,	batch    30,	training loss: 0.463
epoch 81,	batch    40,	training loss: 0.376
epoch 81,	batch    50,	training loss: 0.448
epoch 81,	batch    60,	training loss: 0.330
epoch 81,	batch    70,	training loss: 0.318
epoch 81,	batch    80,	training loss: 0.299
epoch 81,	batch    90,	training loss: 0.377
epoch 81,	batch   100,	training loss: 0.513
epoch 81,	batch   110,	training loss: 0.423
epoch 81,	batch   120,	training loss: 0.352
epoch 81,	batch   130,	training loss: 0.411
epoch 81,	batch   140,	training loss: 0.439
epoch 81,	batch   150,	training loss: 0.350
epoch 81,	batch   160,	training loss: 0.439
epoch 81,	batch   170,	training loss: 0.452
epoch 81,	batch   180,	training loss: 0.374
epoch 81,	batch   190,	training loss: 0.468
epoch 81,	batch   200,	training loss: 0.316
epoch 81,	batch   210,	training loss: 0.325
epoch 81,	batch   220,	training loss: 0.403
epoch 81,	batch   230,	training loss: 0.459
epoch 81,	batch   240,	training loss: 0.440
epoch 81,	batch   250,	training loss: 0.394
epoch 81,	batch   260,	training loss: 0.350
epoch 81,	batch   270,	training loss: 0.357
epoch 81,	batch   280,	training loss: 0.299
epoch 81,	batch   290,	training loss: 0.375
epoch 81,	batch   300,	training loss: 0.257
epoch 81,	batch   310,	training loss: 0.351
end of epoch 81
testing on validation set:
# correct:  12701/20000 = 63.505%
# off by 1: 4869/20000 = 24.345%
epoch 82,	batch    10,	training loss: 0.623
epoch 82,	batch    20,	training loss: 0.539
epoch 82,	batch    30,	training loss: 0.775
epoch 82,	batch    40,	training loss: 0.758
epoch 82,	batch    50,	training loss: 0.723
epoch 82,	batch    60,	training loss: 0.553
epoch 82,	batch    70,	training loss: 0.578
epoch 82,	batch    80,	training loss: 0.400
epoch 82,	batch    90,	training loss: 0.320
epoch 82,	batch   100,	training loss: 0.463
epoch 82,	batch   110,	training loss: 0.365
epoch 82,	batch   120,	training loss: 0.283
epoch 82,	batch   130,	training loss: 0.334
epoch 82,	batch   140,	training loss: 0.324
epoch 82,	batch   150,	training loss: 0.401
epoch 82,	batch   160,	training loss: 0.291
epoch 82,	batch   170,	training loss: 0.360
epoch 82,	batch   180,	training loss: 0.402
epoch 82,	batch   190,	training loss: 0.524
epoch 82,	batch   200,	training loss: 0.463
epoch 82,	batch   210,	training loss: 0.414
epoch 82,	batch   220,	training loss: 0.579
epoch 82,	batch   230,	training loss: 0.790
epoch 82,	batch   240,	training loss: 0.924
epoch 82,	batch   250,	training loss: 0.818
epoch 82,	batch   260,	training loss: 0.529
epoch 82,	batch   270,	training loss: 0.425
epoch 82,	batch   280,	training loss: 0.541
epoch 82,	batch   290,	training loss: 0.528
epoch 82,	batch   300,	training loss: 0.419
epoch 82,	batch   310,	training loss: 0.351
end of epoch 82
testing on validation set:
# correct:  11879/20000 = 59.395%
# off by 1: 5483/20000 = 27.415%
epoch 83,	batch    10,	training loss: 0.518
epoch 83,	batch    20,	training loss: 0.361
epoch 83,	batch    30,	training loss: 0.426
epoch 83,	batch    40,	training loss: 0.447
epoch 83,	batch    50,	training loss: 0.386
epoch 83,	batch    60,	training loss: 0.384
epoch 83,	batch    70,	training loss: 0.385
epoch 83,	batch    80,	training loss: 0.409
epoch 83,	batch    90,	training loss: 0.381
epoch 83,	batch   100,	training loss: 0.307
epoch 83,	batch   110,	training loss: 0.359
epoch 83,	batch   120,	training loss: 0.286
epoch 83,	batch   130,	training loss: 0.453
epoch 83,	batch   140,	training loss: 0.422
epoch 83,	batch   150,	training loss: 0.338
epoch 83,	batch   160,	training loss: 0.328
epoch 83,	batch   170,	training loss: 0.406
epoch 83,	batch   180,	training loss: 0.371
epoch 83,	batch   190,	training loss: 0.422
epoch 83,	batch   200,	training loss: 0.397
epoch 83,	batch   210,	training loss: 0.359
epoch 83,	batch   220,	training loss: 0.409
epoch 83,	batch   230,	training loss: 0.324
epoch 83,	batch   240,	training loss: 0.289
epoch 83,	batch   250,	training loss: 0.491
epoch 83,	batch   260,	training loss: 0.382
epoch 83,	batch   270,	training loss: 0.325
epoch 83,	batch   280,	training loss: 0.362
epoch 83,	batch   290,	training loss: 0.609
epoch 83,	batch   300,	training loss: 0.388
epoch 83,	batch   310,	training loss: 0.425
end of epoch 83
testing on validation set:
# correct:  13431/20000 = 67.155%
# off by 1: 4489/20000 = 22.445%
epoch 84,	batch    10,	training loss: 0.322
epoch 84,	batch    20,	training loss: 0.258
epoch 84,	batch    30,	training loss: 0.269
epoch 84,	batch    40,	training loss: 0.384
epoch 84,	batch    50,	training loss: 0.288
epoch 84,	batch    60,	training loss: 0.277
epoch 84,	batch    70,	training loss: 0.370
epoch 84,	batch    80,	training loss: 0.331
epoch 84,	batch    90,	training loss: 0.276
epoch 84,	batch   100,	training loss: 0.278
epoch 84,	batch   110,	training loss: 0.290
epoch 84,	batch   120,	training loss: 0.342
epoch 84,	batch   130,	training loss: 0.248
epoch 84,	batch   140,	training loss: 0.280
epoch 84,	batch   150,	training loss: 0.394
epoch 84,	batch   160,	training loss: 0.374
epoch 84,	batch   170,	training loss: 0.433
epoch 84,	batch   180,	training loss: 0.386
epoch 84,	batch   190,	training loss: 0.467
epoch 84,	batch   200,	training loss: 0.618
epoch 84,	batch   210,	training loss: 0.490
epoch 84,	batch   220,	training loss: 0.374
epoch 84,	batch   230,	training loss: 0.300
epoch 84,	batch   240,	training loss: 0.362
epoch 84,	batch   250,	training loss: 0.419
epoch 84,	batch   260,	training loss: 0.297
epoch 84,	batch   270,	training loss: 0.412
epoch 84,	batch   280,	training loss: 0.374
epoch 84,	batch   290,	training loss: 0.414
epoch 84,	batch   300,	training loss: 0.308
epoch 84,	batch   310,	training loss: 0.295
end of epoch 84
testing on validation set:
# correct:  12933/20000 = 64.665%
# off by 1: 4723/20000 = 23.615%
epoch 85,	batch    10,	training loss: 0.364
epoch 85,	batch    20,	training loss: 0.614
epoch 85,	batch    30,	training loss: 0.748
epoch 85,	batch    40,	training loss: 0.483
epoch 85,	batch    50,	training loss: 0.403
epoch 85,	batch    60,	training loss: 0.335
epoch 85,	batch    70,	training loss: 0.413
epoch 85,	batch    80,	training loss: 0.367
epoch 85,	batch    90,	training loss: 0.334
epoch 85,	batch   100,	training loss: 0.292
epoch 85,	batch   110,	training loss: 0.429
epoch 85,	batch   120,	training loss: 0.327
epoch 85,	batch   130,	training loss: 0.495
epoch 85,	batch   140,	training loss: 0.482
epoch 85,	batch   150,	training loss: 0.655
epoch 85,	batch   160,	training loss: 0.523
epoch 85,	batch   170,	training loss: 0.559
epoch 85,	batch   180,	training loss: 1.184
epoch 85,	batch   190,	training loss: 0.565
epoch 85,	batch   200,	training loss: 0.467
epoch 85,	batch   210,	training loss: 0.474
epoch 85,	batch   220,	training loss: 0.304
epoch 85,	batch   230,	training loss: 0.317
epoch 85,	batch   240,	training loss: 0.363
epoch 85,	batch   250,	training loss: 0.411
epoch 85,	batch   260,	training loss: 0.423
epoch 85,	batch   270,	training loss: 0.381
epoch 85,	batch   280,	training loss: 0.486
epoch 85,	batch   290,	training loss: 0.635
epoch 85,	batch   300,	training loss: 0.391
epoch 85,	batch   310,	training loss: 0.381
end of epoch 85
testing on validation set:
# correct:  12775/20000 = 63.875%
# off by 1: 4937/20000 = 24.685%
epoch 86,	batch    10,	training loss: 0.475
epoch 86,	batch    20,	training loss: 0.476
epoch 86,	batch    30,	training loss: 0.524
epoch 86,	batch    40,	training loss: 0.466
epoch 86,	batch    50,	training loss: 0.378
epoch 86,	batch    60,	training loss: 0.377
epoch 86,	batch    70,	training loss: 0.258
epoch 86,	batch    80,	training loss: 0.331
epoch 86,	batch    90,	training loss: 0.354
epoch 86,	batch   100,	training loss: 0.823
epoch 86,	batch   110,	training loss: 0.656
epoch 86,	batch   120,	training loss: 0.619
epoch 86,	batch   130,	training loss: 0.485
epoch 86,	batch   140,	training loss: 0.463
epoch 86,	batch   150,	training loss: 0.392
epoch 86,	batch   160,	training loss: 0.456
epoch 86,	batch   170,	training loss: 0.426
epoch 86,	batch   180,	training loss: 0.709
epoch 86,	batch   190,	training loss: 0.583
epoch 86,	batch   200,	training loss: 0.518
epoch 86,	batch   210,	training loss: 0.504
epoch 86,	batch   220,	training loss: 0.378
epoch 86,	batch   230,	training loss: 0.511
epoch 86,	batch   240,	training loss: 0.484
epoch 86,	batch   250,	training loss: 0.648
epoch 86,	batch   260,	training loss: 0.419
epoch 86,	batch   270,	training loss: 0.373
epoch 86,	batch   280,	training loss: 0.350
epoch 86,	batch   290,	training loss: 0.411
epoch 86,	batch   300,	training loss: 0.318
epoch 86,	batch   310,	training loss: 0.353
end of epoch 86
testing on validation set:
# correct:  12788/20000 = 63.94%
# off by 1: 4882/20000 = 24.41%
epoch 87,	batch    10,	training loss: 0.361
epoch 87,	batch    20,	training loss: 0.374
epoch 87,	batch    30,	training loss: 0.308
epoch 87,	batch    40,	training loss: 0.268
epoch 87,	batch    50,	training loss: 0.311
epoch 87,	batch    60,	training loss: 0.241
epoch 87,	batch    70,	training loss: 0.362
epoch 87,	batch    80,	training loss: 0.454
epoch 87,	batch    90,	training loss: 0.262
epoch 87,	batch   100,	training loss: 0.397
epoch 87,	batch   110,	training loss: 0.342
epoch 87,	batch   120,	training loss: 0.340
epoch 87,	batch   130,	training loss: 0.285
epoch 87,	batch   140,	training loss: 0.403
epoch 87,	batch   150,	training loss: 0.360
epoch 87,	batch   160,	training loss: 0.663
epoch 87,	batch   170,	training loss: 0.811
epoch 87,	batch   180,	training loss: 0.684
epoch 87,	batch   190,	training loss: 0.515
epoch 87,	batch   200,	training loss: 0.571
epoch 87,	batch   210,	training loss: 0.381
epoch 87,	batch   220,	training loss: 0.341
epoch 87,	batch   230,	training loss: 0.414
epoch 87,	batch   240,	training loss: 0.614
epoch 87,	batch   250,	training loss: 0.409
epoch 87,	batch   260,	training loss: 0.383
epoch 87,	batch   270,	training loss: 0.390
epoch 87,	batch   280,	training loss: 0.342
epoch 87,	batch   290,	training loss: 0.320
epoch 87,	batch   300,	training loss: 0.334
epoch 87,	batch   310,	training loss: 0.444
end of epoch 87
testing on validation set:
# correct:  12654/20000 = 63.27%
# off by 1: 4949/20000 = 24.745%
epoch 88,	batch    10,	training loss: 0.459
epoch 88,	batch    20,	training loss: 0.393
epoch 88,	batch    30,	training loss: 0.348
epoch 88,	batch    40,	training loss: 0.296
epoch 88,	batch    50,	training loss: 0.410
epoch 88,	batch    60,	training loss: 0.591
epoch 88,	batch    70,	training loss: 0.389
epoch 88,	batch    80,	training loss: 0.461
epoch 88,	batch    90,	training loss: 0.903
epoch 88,	batch   100,	training loss: 0.828
epoch 88,	batch   110,	training loss: 0.560
epoch 88,	batch   120,	training loss: 0.441
epoch 88,	batch   130,	training loss: 0.384
epoch 88,	batch   140,	training loss: 0.323
epoch 88,	batch   150,	training loss: 0.413
epoch 88,	batch   160,	training loss: 0.380
epoch 88,	batch   170,	training loss: 0.367
epoch 88,	batch   180,	training loss: 0.273
epoch 88,	batch   190,	training loss: 0.463
epoch 88,	batch   200,	training loss: 0.314
epoch 88,	batch   210,	training loss: 0.562
epoch 88,	batch   220,	training loss: 0.451
epoch 88,	batch   230,	training loss: 0.456
epoch 88,	batch   240,	training loss: 0.419
epoch 88,	batch   250,	training loss: 0.420
epoch 88,	batch   260,	training loss: 0.276
epoch 88,	batch   270,	training loss: 0.269
epoch 88,	batch   280,	training loss: 0.291
epoch 88,	batch   290,	training loss: 0.469
epoch 88,	batch   300,	training loss: 0.942
epoch 88,	batch   310,	training loss: 0.664
end of epoch 88
testing on validation set:
# correct:  13382/20000 = 66.91%
# off by 1: 4737/20000 = 23.685%
epoch 89,	batch    10,	training loss: 0.593
epoch 89,	batch    20,	training loss: 0.375
epoch 89,	batch    30,	training loss: 0.639
epoch 89,	batch    40,	training loss: 0.417
epoch 89,	batch    50,	training loss: 0.492
epoch 89,	batch    60,	training loss: 0.503
epoch 89,	batch    70,	training loss: 0.432
epoch 89,	batch    80,	training loss: 0.464
epoch 89,	batch    90,	training loss: 0.390
epoch 89,	batch   100,	training loss: 0.422
epoch 89,	batch   110,	training loss: 0.381
epoch 89,	batch   120,	training loss: 0.416
epoch 89,	batch   130,	training loss: 0.572
epoch 89,	batch   140,	training loss: 0.695
epoch 89,	batch   150,	training loss: 0.573
epoch 89,	batch   160,	training loss: 0.556
epoch 89,	batch   170,	training loss: 0.521
epoch 89,	batch   180,	training loss: 0.549
epoch 89,	batch   190,	training loss: 0.311
epoch 89,	batch   200,	training loss: 0.457
epoch 89,	batch   210,	training loss: 0.562
epoch 89,	batch   220,	training loss: 0.705
epoch 89,	batch   230,	training loss: 0.416
epoch 89,	batch   240,	training loss: 0.293
epoch 89,	batch   250,	training loss: 0.425
epoch 89,	batch   260,	training loss: 0.410
epoch 89,	batch   270,	training loss: 0.292
epoch 89,	batch   280,	training loss: 0.337
epoch 89,	batch   290,	training loss: 0.322
epoch 89,	batch   300,	training loss: 0.356
epoch 89,	batch   310,	training loss: 0.363
end of epoch 89
testing on validation set:
# correct:  13220/20000 = 66.1%
# off by 1: 4623/20000 = 23.115%
epoch 90,	batch    10,	training loss: 0.391
epoch 90,	batch    20,	training loss: 0.425
epoch 90,	batch    30,	training loss: 0.375
epoch 90,	batch    40,	training loss: 0.412
epoch 90,	batch    50,	training loss: 0.377
epoch 90,	batch    60,	training loss: 0.376
epoch 90,	batch    70,	training loss: 0.382
epoch 90,	batch    80,	training loss: 0.331
epoch 90,	batch    90,	training loss: 0.309
epoch 90,	batch   100,	training loss: 0.360
epoch 90,	batch   110,	training loss: 0.438
epoch 90,	batch   120,	training loss: 0.403
epoch 90,	batch   130,	training loss: 0.407
epoch 90,	batch   140,	training loss: 0.467
epoch 90,	batch   150,	training loss: 0.331
epoch 90,	batch   160,	training loss: 0.398
epoch 90,	batch   170,	training loss: 1.015
epoch 90,	batch   180,	training loss: 0.582
epoch 90,	batch   190,	training loss: 0.989
epoch 90,	batch   200,	training loss: 1.633
epoch 90,	batch   210,	training loss: 1.244
epoch 90,	batch   220,	training loss: 1.296
epoch 90,	batch   230,	training loss: 0.771
epoch 90,	batch   240,	training loss: 0.809
epoch 90,	batch   250,	training loss: 0.522
epoch 90,	batch   260,	training loss: 0.455
epoch 90,	batch   270,	training loss: 0.448
epoch 90,	batch   280,	training loss: 0.359
epoch 90,	batch   290,	training loss: 0.398
epoch 90,	batch   300,	training loss: 0.458
epoch 90,	batch   310,	training loss: 0.875
end of epoch 90
testing on validation set:
# correct:  12902/20000 = 64.51%
# off by 1: 4709/20000 = 23.545%
epoch 91,	batch    10,	training loss: 0.883
epoch 91,	batch    20,	training loss: 0.415
epoch 91,	batch    30,	training loss: 0.506
epoch 91,	batch    40,	training loss: 0.469
epoch 91,	batch    50,	training loss: 0.371
epoch 91,	batch    60,	training loss: 0.299
epoch 91,	batch    70,	training loss: 0.282
epoch 91,	batch    80,	training loss: 0.383
epoch 91,	batch    90,	training loss: 0.561
epoch 91,	batch   100,	training loss: 0.525
epoch 91,	batch   110,	training loss: 0.316
epoch 91,	batch   120,	training loss: 0.297
epoch 91,	batch   130,	training loss: 0.307
epoch 91,	batch   140,	training loss: 0.388
epoch 91,	batch   150,	training loss: 0.398
epoch 91,	batch   160,	training loss: 0.314
epoch 91,	batch   170,	training loss: 0.356
epoch 91,	batch   180,	training loss: 0.417
epoch 91,	batch   190,	training loss: 0.403
epoch 91,	batch   200,	training loss: 0.656
epoch 91,	batch   210,	training loss: 0.537
epoch 91,	batch   220,	training loss: 0.328
epoch 91,	batch   230,	training loss: 0.361
epoch 91,	batch   240,	training loss: 0.326
epoch 91,	batch   250,	training loss: 0.330
epoch 91,	batch   260,	training loss: 0.337
epoch 91,	batch   270,	training loss: 0.290
epoch 91,	batch   280,	training loss: 0.886
epoch 91,	batch   290,	training loss: 1.087
epoch 91,	batch   300,	training loss: 1.211
epoch 91,	batch   310,	training loss: 0.661
end of epoch 91
testing on validation set:
# correct:  13300/20000 = 66.5%
# off by 1: 4704/20000 = 23.52%
epoch 92,	batch    10,	training loss: 0.404
epoch 92,	batch    20,	training loss: 0.311
epoch 92,	batch    30,	training loss: 0.423
epoch 92,	batch    40,	training loss: 0.523
epoch 92,	batch    50,	training loss: 0.422
epoch 92,	batch    60,	training loss: 0.362
epoch 92,	batch    70,	training loss: 0.417
epoch 92,	batch    80,	training loss: 0.577
epoch 92,	batch    90,	training loss: 0.546
epoch 92,	batch   100,	training loss: 0.414
epoch 92,	batch   110,	training loss: 0.379
epoch 92,	batch   120,	training loss: 0.351
epoch 92,	batch   130,	training loss: 0.339
epoch 92,	batch   140,	training loss: 0.336
epoch 92,	batch   150,	training loss: 0.450
epoch 92,	batch   160,	training loss: 0.478
epoch 92,	batch   170,	training loss: 0.353
epoch 92,	batch   180,	training loss: 0.446
epoch 92,	batch   190,	training loss: 0.296
epoch 92,	batch   200,	training loss: 0.467
epoch 92,	batch   210,	training loss: 0.577
epoch 92,	batch   220,	training loss: 0.518
epoch 92,	batch   230,	training loss: 0.333
epoch 92,	batch   240,	training loss: 0.380
epoch 92,	batch   250,	training loss: 0.353
epoch 92,	batch   260,	training loss: 0.739
epoch 92,	batch   270,	training loss: 0.437
epoch 92,	batch   280,	training loss: 0.338
epoch 92,	batch   290,	training loss: 0.325
epoch 92,	batch   300,	training loss: 0.374
epoch 92,	batch   310,	training loss: 0.336
end of epoch 92
testing on validation set:
# correct:  13807/20000 = 69.035%
# off by 1: 4279/20000 = 21.395%
epoch 93,	batch    10,	training loss: 0.409
epoch 93,	batch    20,	training loss: 0.386
epoch 93,	batch    30,	training loss: 0.433
epoch 93,	batch    40,	training loss: 0.308
epoch 93,	batch    50,	training loss: 0.252
epoch 93,	batch    60,	training loss: 0.358
epoch 93,	batch    70,	training loss: 0.450
epoch 93,	batch    80,	training loss: 0.495
epoch 93,	batch    90,	training loss: 0.338
epoch 93,	batch   100,	training loss: 0.311
epoch 93,	batch   110,	training loss: 0.333
epoch 93,	batch   120,	training loss: 0.544
epoch 93,	batch   130,	training loss: 0.350
epoch 93,	batch   140,	training loss: 0.322
epoch 93,	batch   150,	training loss: 0.376
epoch 93,	batch   160,	training loss: 0.373
epoch 93,	batch   170,	training loss: 0.299
epoch 93,	batch   180,	training loss: 0.293
epoch 93,	batch   190,	training loss: 0.321
epoch 93,	batch   200,	training loss: 0.441
epoch 93,	batch   210,	training loss: 0.326
epoch 93,	batch   220,	training loss: 0.267
epoch 93,	batch   230,	training loss: 0.384
epoch 93,	batch   240,	training loss: 0.319
epoch 93,	batch   250,	training loss: 0.539
epoch 93,	batch   260,	training loss: 0.443
epoch 93,	batch   270,	training loss: 0.399
epoch 93,	batch   280,	training loss: 0.439
epoch 93,	batch   290,	training loss: 0.858
epoch 93,	batch   300,	training loss: 0.531
epoch 93,	batch   310,	training loss: 0.311
end of epoch 93
testing on validation set:
# correct:  13930/20000 = 69.65%
# off by 1: 4289/20000 = 21.445%
epoch 94,	batch    10,	training loss: 0.353
epoch 94,	batch    20,	training loss: 0.408
epoch 94,	batch    30,	training loss: 0.326
epoch 94,	batch    40,	training loss: 0.364
epoch 94,	batch    50,	training loss: 0.412
epoch 94,	batch    60,	training loss: 0.387
epoch 94,	batch    70,	training loss: 0.361
epoch 94,	batch    80,	training loss: 0.758
epoch 94,	batch    90,	training loss: 0.519
epoch 94,	batch   100,	training loss: 0.336
epoch 94,	batch   110,	training loss: 0.434
epoch 94,	batch   120,	training loss: 0.688
epoch 94,	batch   130,	training loss: 0.887
epoch 94,	batch   140,	training loss: 0.789
epoch 94,	batch   150,	training loss: 0.868
epoch 94,	batch   160,	training loss: 0.740
epoch 94,	batch   170,	training loss: 0.560
epoch 94,	batch   180,	training loss: 0.493
epoch 94,	batch   190,	training loss: 0.693
epoch 94,	batch   200,	training loss: 0.392
epoch 94,	batch   210,	training loss: 0.338
epoch 94,	batch   220,	training loss: 0.456
epoch 94,	batch   230,	training loss: 0.317
epoch 94,	batch   240,	training loss: 0.504
epoch 94,	batch   250,	training loss: 0.693
epoch 94,	batch   260,	training loss: 0.778
epoch 94,	batch   270,	training loss: 1.081
epoch 94,	batch   280,	training loss: 0.797
epoch 94,	batch   290,	training loss: 0.713
epoch 94,	batch   300,	training loss: 0.859
epoch 94,	batch   310,	training loss: 0.957
end of epoch 94
testing on validation set:
# correct:  13447/20000 = 67.235%
# off by 1: 4555/20000 = 22.775%
epoch 95,	batch    10,	training loss: 0.401
epoch 95,	batch    20,	training loss: 0.442
epoch 95,	batch    30,	training loss: 0.583
epoch 95,	batch    40,	training loss: 0.447
epoch 95,	batch    50,	training loss: 0.464
epoch 95,	batch    60,	training loss: 0.320
epoch 95,	batch    70,	training loss: 0.274
epoch 95,	batch    80,	training loss: 0.387
epoch 95,	batch    90,	training loss: 0.267
epoch 95,	batch   100,	training loss: 0.279
epoch 95,	batch   110,	training loss: 0.236
epoch 95,	batch   120,	training loss: 0.282
epoch 95,	batch   130,	training loss: 0.275
epoch 95,	batch   140,	training loss: 0.302
epoch 95,	batch   150,	training loss: 0.483
epoch 95,	batch   160,	training loss: 0.515
epoch 95,	batch   170,	training loss: 0.403
epoch 95,	batch   180,	training loss: 0.396
epoch 95,	batch   190,	training loss: 0.281
epoch 95,	batch   200,	training loss: 0.360
epoch 95,	batch   210,	training loss: 0.308
epoch 95,	batch   220,	training loss: 0.548
epoch 95,	batch   230,	training loss: 1.242
epoch 95,	batch   240,	training loss: 0.741
epoch 95,	batch   250,	training loss: 0.589
epoch 95,	batch   260,	training loss: 0.335
epoch 95,	batch   270,	training loss: 0.398
epoch 95,	batch   280,	training loss: 0.565
epoch 95,	batch   290,	training loss: 0.489
epoch 95,	batch   300,	training loss: 0.488
epoch 95,	batch   310,	training loss: 0.424
end of epoch 95
testing on validation set:
# correct:  13017/20000 = 65.085%
# off by 1: 4770/20000 = 23.85%
epoch 96,	batch    10,	training loss: 0.474
epoch 96,	batch    20,	training loss: 0.339
epoch 96,	batch    30,	training loss: 0.395
epoch 96,	batch    40,	training loss: 0.315
epoch 96,	batch    50,	training loss: 0.339
epoch 96,	batch    60,	training loss: 0.496
epoch 96,	batch    70,	training loss: 0.437
epoch 96,	batch    80,	training loss: 0.354
epoch 96,	batch    90,	training loss: 0.356
epoch 96,	batch   100,	training loss: 0.415
epoch 96,	batch   110,	training loss: 0.302
epoch 96,	batch   120,	training loss: 0.316
epoch 96,	batch   130,	training loss: 0.292
epoch 96,	batch   140,	training loss: 0.518
epoch 96,	batch   150,	training loss: 0.594
epoch 96,	batch   160,	training loss: 0.649
epoch 96,	batch   170,	training loss: 0.389
epoch 96,	batch   180,	training loss: 0.556
epoch 96,	batch   190,	training loss: 0.423
epoch 96,	batch   200,	training loss: 0.336
epoch 96,	batch   210,	training loss: 0.306
epoch 96,	batch   220,	training loss: 0.301
epoch 96,	batch   230,	training loss: 0.340
epoch 96,	batch   240,	training loss: 0.391
epoch 96,	batch   250,	training loss: 0.380
epoch 96,	batch   260,	training loss: 0.404
epoch 96,	batch   270,	training loss: 0.335
epoch 96,	batch   280,	training loss: 0.407
epoch 96,	batch   290,	training loss: 0.487
epoch 96,	batch   300,	training loss: 0.362
epoch 96,	batch   310,	training loss: 0.335
end of epoch 96
testing on validation set:
# correct:  13867/20000 = 69.335%
# off by 1: 4264/20000 = 21.32%
epoch 97,	batch    10,	training loss: 0.401
epoch 97,	batch    20,	training loss: 0.472
epoch 97,	batch    30,	training loss: 0.519
epoch 97,	batch    40,	training loss: 0.707
epoch 97,	batch    50,	training loss: 0.544
epoch 97,	batch    60,	training loss: 0.454
epoch 97,	batch    70,	training loss: 0.397
epoch 97,	batch    80,	training loss: 0.443
epoch 97,	batch    90,	training loss: 0.664
epoch 97,	batch   100,	training loss: 0.611
epoch 97,	batch   110,	training loss: 0.393
epoch 97,	batch   120,	training loss: 0.270
epoch 97,	batch   130,	training loss: 0.556
epoch 97,	batch   140,	training loss: 0.638
epoch 97,	batch   150,	training loss: 0.681
epoch 97,	batch   160,	training loss: 0.633
epoch 97,	batch   170,	training loss: 0.606
epoch 97,	batch   180,	training loss: 0.827
epoch 97,	batch   190,	training loss: 0.388
epoch 97,	batch   200,	training loss: 0.457
epoch 97,	batch   210,	training loss: 0.441
epoch 97,	batch   220,	training loss: 0.275
epoch 97,	batch   230,	training loss: 0.349
epoch 97,	batch   240,	training loss: 0.408
epoch 97,	batch   250,	training loss: 0.331
epoch 97,	batch   260,	training loss: 0.280
epoch 97,	batch   270,	training loss: 0.400
epoch 97,	batch   280,	training loss: 0.437
epoch 97,	batch   290,	training loss: 0.316
epoch 97,	batch   300,	training loss: 0.346
epoch 97,	batch   310,	training loss: 0.239
end of epoch 97
testing on validation set:
# correct:  12730/20000 = 63.65%
# off by 1: 5065/20000 = 25.325%
epoch 98,	batch    10,	training loss: 0.453
epoch 98,	batch    20,	training loss: 0.306
epoch 98,	batch    30,	training loss: 0.287
epoch 98,	batch    40,	training loss: 0.421
epoch 98,	batch    50,	training loss: 0.482
epoch 98,	batch    60,	training loss: 0.379
epoch 98,	batch    70,	training loss: 0.327
epoch 98,	batch    80,	training loss: 0.572
epoch 98,	batch    90,	training loss: 0.288
epoch 98,	batch   100,	training loss: 0.337
epoch 98,	batch   110,	training loss: 0.332
epoch 98,	batch   120,	training loss: 0.378
epoch 98,	batch   130,	training loss: 0.358
epoch 98,	batch   140,	training loss: 0.515
epoch 98,	batch   150,	training loss: 0.444
epoch 98,	batch   160,	training loss: 0.343
epoch 98,	batch   170,	training loss: 0.568
epoch 98,	batch   180,	training loss: 0.558
epoch 98,	batch   190,	training loss: 0.371
epoch 98,	batch   200,	training loss: 0.462
epoch 98,	batch   210,	training loss: 0.725
epoch 98,	batch   220,	training loss: 0.517
epoch 98,	batch   230,	training loss: 0.452
epoch 98,	batch   240,	training loss: 0.339
epoch 98,	batch   250,	training loss: 0.347
epoch 98,	batch   260,	training loss: 0.387
epoch 98,	batch   270,	training loss: 0.566
epoch 98,	batch   280,	training loss: 0.483
epoch 98,	batch   290,	training loss: 0.336
epoch 98,	batch   300,	training loss: 0.258
epoch 98,	batch   310,	training loss: 0.387
end of epoch 98
testing on validation set:
# correct:  12366/20000 = 61.83%
# off by 1: 5032/20000 = 25.16%
epoch 99,	batch    10,	training loss: 0.510
epoch 99,	batch    20,	training loss: 0.267
epoch 99,	batch    30,	training loss: 0.256
epoch 99,	batch    40,	training loss: 0.387
epoch 99,	batch    50,	training loss: 0.314
epoch 99,	batch    60,	training loss: 0.362
epoch 99,	batch    70,	training loss: 0.326
epoch 99,	batch    80,	training loss: 0.243
epoch 99,	batch    90,	training loss: 0.269
epoch 99,	batch   100,	training loss: 0.286
epoch 99,	batch   110,	training loss: 0.379
epoch 99,	batch   120,	training loss: 0.387
epoch 99,	batch   130,	training loss: 0.250
epoch 99,	batch   140,	training loss: 0.272
epoch 99,	batch   150,	training loss: 0.386
epoch 99,	batch   160,	training loss: 0.597
epoch 99,	batch   170,	training loss: 0.346
epoch 99,	batch   180,	training loss: 0.350
epoch 99,	batch   190,	training loss: 0.355
epoch 99,	batch   200,	training loss: 0.470
epoch 99,	batch   210,	training loss: 0.396
epoch 99,	batch   220,	training loss: 0.724
epoch 99,	batch   230,	training loss: 0.430
epoch 99,	batch   240,	training loss: 0.307
epoch 99,	batch   250,	training loss: 0.319
epoch 99,	batch   260,	training loss: 0.254
epoch 99,	batch   270,	training loss: 0.231
epoch 99,	batch   280,	training loss: 0.276
epoch 99,	batch   290,	training loss: 0.249
epoch 99,	batch   300,	training loss: 0.250
epoch 99,	batch   310,	training loss: 0.261
end of epoch 99
testing on validation set:
# correct:  13768/20000 = 68.84%
# off by 1: 4319/20000 = 21.595%
epoch 100,	batch    10,	training loss: 0.323
epoch 100,	batch    20,	training loss: 0.527
epoch 100,	batch    30,	training loss: 0.422
epoch 100,	batch    40,	training loss: 0.282
epoch 100,	batch    50,	training loss: 0.266
epoch 100,	batch    60,	training loss: 0.315
epoch 100,	batch    70,	training loss: 0.405
epoch 100,	batch    80,	training loss: 0.350
epoch 100,	batch    90,	training loss: 0.451
epoch 100,	batch   100,	training loss: 0.430
epoch 100,	batch   110,	training loss: 0.428
epoch 100,	batch   120,	training loss: 0.337
epoch 100,	batch   130,	training loss: 0.443
epoch 100,	batch   140,	training loss: 0.342
epoch 100,	batch   150,	training loss: 0.240
epoch 100,	batch   160,	training loss: 0.290
epoch 100,	batch   170,	training loss: 0.471
epoch 100,	batch   180,	training loss: 0.420
epoch 100,	batch   190,	training loss: 0.414
epoch 100,	batch   200,	training loss: 0.614
epoch 100,	batch   210,	training loss: 0.568
epoch 100,	batch   220,	training loss: 0.843
epoch 100,	batch   230,	training loss: 0.399
epoch 100,	batch   240,	training loss: 0.534
epoch 100,	batch   250,	training loss: 0.450
epoch 100,	batch   260,	training loss: 0.851
epoch 100,	batch   270,	training loss: 0.411
epoch 100,	batch   280,	training loss: 0.598
epoch 100,	batch   290,	training loss: 0.857
epoch 100,	batch   300,	training loss: 0.837
epoch 100,	batch   310,	training loss: 0.373
end of epoch 100
testing on validation set:
# correct:  13615/20000 = 68.075%
# off by 1: 4509/20000 = 22.545%
Finished training
Testing network on test set
# correct:  13611/20000 = 68.055%
6847 errors saved to ./report_regression/errors/
predicted box in blue, correct in green
